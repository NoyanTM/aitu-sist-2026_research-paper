% General styles and packages: (@TODO: move preamble to separate file)
\documentclass[conference, comsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
\usepackage[none]{hyphenat} % turn off hyphenation
\setlength{\parindent}{0pt} % no space between paragraphs
\graphicspath{{figures/}} % default path for figures

% References: (@TODO: migrate to bibtex because IEEETran has not fully correct styling for biblatex)
\usepackage[backend=biber, style=ieee, sorting=none, url=false]{biblatex}
\addbibresource{references.bib}
%% IEEE bibliography styles for biblatex:
\renewcommand*{\bibfont}{\footnotesize}
\setlength{\biblabelsep}{\labelsep}
\setlength{\bibitemsep}{\IEEEbibitemsep}

% Heading: (@TODO: \usepackage{fancyhdr})
\makeatletter
\def\ps@IEEEtitlepagestyle{
  \def\@oddhead{\parbox{\textwidth}{
    {\footnotesize
    2026 IEEE 6th International Conference on Smart Information Systems and Technologies (SIST)\\
    13--15 May 2025, Astana, Kazakhstan\par}
  }}
  \def\@oddfoot{}
}
\makeatother

\begin{document}
\title{Customizable methodology for in-depth analysis of software within isolated computational environments to secure critical IT infrastructures\\}

\author{\IEEEauthorblockN{1\textsuperscript{st} Noyan Tendikov}
\IEEEauthorblockA{\textit{School of Cybersecurity} \\
\textit{Astana IT University}\\
Astana, Kazakhstan \\
242710@astanait.edu.kz \\
ORCID: 0009-0009-7251-8830}
} % @TODO: add rzaeva and lishnevsky to authors?

\maketitle

\begin{abstract}
Critical IT infrastructures are increasingly threatened by hidden attacks targeting their components. Malicious actors can infiltrate silently through software dependencies by launching an indirect attack on internal vulnerabilities. This research aims to explore this issue by focusing on software composition and its unification, extending the current approaches through a prototype framework for analyzing software in specialized isolated environments. The preliminary results of risk assessment and threat modeling demonstrate its viability and security properties. Further refinement and studies are required, including a reference implementation, a comprehensive technical specification, and iterative field research with detailed statistics based on the implemented approach.
\end{abstract}

\begin{IEEEkeywords}
software supply chain; infrastructure-as-code; critical IT infrastructure; software composition analysis; software dependencies; software bill of materials; sandboxing; malware and binary analysis; digital twins; intelligent agents
\end{IEEEkeywords}

\section{Introduction}
% @TODO: Refine introduction to be whole page without and more solid (цельным повестованием as flow)
% @TODO: Add more details about research goal, hypotheses, research questions, scope of the work, brief details about previous works in this field of research, emphasize on new advances, theories and/or applications and include an analysis of results and findings

% subpart: current sutation about escalation of conflicts that affect on cybersecurity of critical infrastructures
Modern cyber conflicts have escalated into a condition of constant cyberwar, driven by countless threats. Unlike traditional military conflicts, this warfare is taking place within a digital space and often goes unnoticed by the general public due to their lack of awareness and competence \cite{almeidaComparativeAnalysisEUbased2025}. The current policy of major states is dictated by the principle of "peace through strength", prioritizing the rule of power over ethical considerations and international law. It leads to an intensified arms race between multiple actors, such as government agencies, armies, corporations, and private groups. Under these circumstances of ongoing confrontations between defenders and attackers, ensuring security is only a continuous process rather than an absolute guarantee \cite{safitraCounterattackingCyberThreats2023}. So, the industry of information technology (IT) should be engaged in systematic risk assessment and apply reactive patches as problems emerge, because the number of potential threats and vulnerabilities is inherently unknown and cannot be fully identified or eliminated in upfront. Moreover, the increasing rate of automation and digitalization exposes associated risks for critical infrastructure, where their compromise could create dangerous situations and collateral damage for citizens \cite[pp. 1--2]{nationalinstituteofstandardsandtechnologyFrameworkImprovingCritical2018}. For example, the Cybersecurity and Infrastructure Security Agency (CISA) of the USA adopted a classification that identifies 16 critical infrastructure sectors \cite[pp. 6--7]{InfrastructureResiliencePlanning}, including the context of IT infrastructures. This sector is composed from corresponding services and assets (e.g., maintenance personnel, electricity, networks, equipment, hardware, software), providing an operational basis or computational environment to utilize for information systems (IS) \cite{qatawnehMediatingRoleTechnological2024}.

% subpart: reasons and factors of attacks on critical infrastructure and their consequences
Fundamentally, attacks targeting critical IT infrastructure are driven by diverse motives and reasons, overlapping with other industries \cite{lehtoCyberattacksCriticalInfrastructure2022}. Historical events have already shown that conflicts of national interests might trigger a security breach on highly protected facilities (e.g., nuclear power plants) \cite{makrakisIndustrialCriticalInfrastructure2021}. For instance, Riggs et al. examined statistics on global cyberattacks against critical infrastructures since 2006 and developed a prediction model that shows exponential growth of incidents with the military sector being the most frequently targeted \cite{riggsImpactVulnerabilitiesMitigation2023}. Another factor that may escalate the situation is the growing trend around generative artificial intelligence (AI), which increases the demand on data centers, stimulating competitors and other third parties to compromise or abuse that computational power for their own benefit \cite{yigitCriticalInfrastructureProtection2024}. Additionally, the increasing interconnectivity of IS, especially through Internet of Things (IoT) devices, causes fragility by exposing multiple points of failure and allows malicious actors or even device manufacturers to gain remote access to them \cite{vardakisReviewSmarthomeSecurity2024, rauhalaPhysicalWeaponizationSmartphone2022}.

% subpart: research goal and structure of the study
Therefore, this study attempts to construct security measures for the IT infrastructure, emphasizing the process of collecting and accounting of its internal resources or components for reliability and early insider threat prevention. Firstly, the literature review covers the historical development of IT infrastructure, complexity management, and issues with software composition. Secondly, the methodology specifies the hypotheses and provides a framework for conducting customizable analyses within sandboxed environments. Thirdly, the results part examines features of the proposed solution. Lastly, the conclusion summarizes findings and potential drafts for future work.

\section{Literature review}
\subsection{Historical development of IT infrastructure}

% subpart: commercialization of IT
The computing ecosystem and its service models have evolved gradually to on-premises and cloud paradigms, becoming increasingly commercialized to meet customer demands and scaling issues, as broadly represented in Figure \ref{fig:developmental_milestones_of_computing_environment}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_1}
    \caption{Developmental milestones of computing environments (VM - virtual machine; C - container; PaaS, IaaS, SaaS - external services): (1) Mainframe. (2) Cluster of servers. (3) Geographically distributed servers with virtualization (visually represented by color difference). (4) - Hybrid approach with cloud services.}
    \label{fig:developmental_milestones_of_computing_environment}
\end{figure}

% subpart: growing ecosystem of IT infrastructure
Initially, there were only mainframes that required dedicated teams for maintenance and performed at a highly mechanical level, so their bulkiness and slow speed enforced considerations for the adaptation of distributed computations \cite[Sec. 3.1]{lindsayEvolutionDistributedComputing2021}. Fortunately, improvements in IT manufacturing significantly contributed to emergence of dedicated servers that could be networked together in a modular way to provide scalability. Following this trend, virtualization moved beyond bare-metal deployments by allowing isolation layers to securely run multiple virtual machines (VMs) on shared hardware to significantly enhance efficiency of resource utilization \cite{randalIdealRealRevisiting2021}. Eventually, increasing demand for international connectivity and economic globalization naturally formed large-scale data centers with network of geographically distributed servers in order to optimize content delivery and processing \cite[pp. 9--11]{lindsayEvolutionDistributedComputing2021}. 

% subpart: "as-a-service" models
Nowadays, organizations can offload many responsibilities and parts of their IT infrastructure to cloud and service providers which abstract physical machines and offer instant on-demand resource provisioning and management, thereby reducing operational burdens via "as-a-service" models with specialized offerings: software as a service (SaaS), platform as a service (PaaS), infrastructure as a service (IaaS) \cite{mellNISTDefinitionCloud2011}. Software engineers often take that for granted, underestimating interdependence of software and hardware sides for facilitating the execution of general-purpose tasks within multiple layers of abstraction. Such service-based approach for outsourcing still requires integrating and coordinating operations over delegated parts (e.g., security policies, privacy compliance, and external monitoring). Moreover, certain strategic systems (i.e., governmental and military) and their data are prohibited from being hosted in the cloud due to secrecy laws and regional restrictions \cite{arifComprehensiveSurveyPrivacyEnhancing2025, alghofailiSecureCloudInfrastructure2021}.

\subsection{Complexity management with codification and modeling}

% subpart: codification that tries to solve complexity and formed Iac
As the sector of IT infrastructure has expanded, its complexity and associated challenges have emerged accordingly. Nonetheless, this complexity has become the primary factor for the process automation against manual, repetitive, and impractical operations. Therefore, automation and orchestration are no longer optional - they are critical for leveraging fully scalable and reliable systems from a strategic perspective. One of the natural strategies for complexity management is code. The process of codification captures procedures, routines, or algorithms of actions in a textual format \cite[pp. 1--2]{ouriquesRoleKnowledgebasedResources2023}. For example, Chuprikov et al. propose specifying security policies in IT systems in a form of domain specific language (DSL) that can be ran within an execution engine \cite{chuprikovSecurityPolicyCode2025}. Codification also established the paradigm of Infrastructure-as-Code (IaC) which is focused on specifying the state and content of IT infrastructures to enable reproducibility (e.g., provisioning, replication, copying, scaling) and modification (with emphasis on transparency and maintainability), as well as management and maintenance by self-documenting code or files that incorporate defined metadata, configurations, and procedures \cite{pahlInfrastructureCodeTechnology2025}. It minimizes human errors and establishes immutable infrastructure with the phoenix model, allowing any server instance to be recreated, replaced, updated, or destroyed from code without losing critical configurations. This approach eliminates the existence of snowflake servers, which are deviated systems with configuration drift and unpredictability due to their manual modification and absence of documentation \cite[Ch. 2]{morrisInfrastructureCodeDynamic2020}.

% subpart: how IaC tools operate
IaC operations are managed by system administrators, DevOps teams (Development and Operations), SREs (Site Reliability Engineers), or IT professionals via IaC toolkits (e.g., Terraform, Ansible), enforcing the utilization of definition files (e.g., templates, playbooks, manifests), configurations, or execution steps for servers, networks, databases, installations, updates, backups, and so on \cite[Ch. 4]{morrisInfrastructureCodeDynamic2020}. They might be implemented internally via general-purpose languages, but using configurations as application programming interfaces (APIs). For example, containerization tools like Docker use Dockerfiles to define application dependencies and versions, while the runtime builds a container with artifacts according to specified requirements, as shown in Figure \ref{fig:self_documenting_system}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_2}
    \caption{Processes of applying changes to servers: (1) Manually. (2) IaC pipeline as a self-documenting system.}
    \label{fig:self_documenting_system}
\end{figure}

% subpart: secure coding practices for IaC configurations
Any IaC code should be proceeded through the continuous integration and continuous delivery (CI/CD) pipeline, which maintains safe deployment by runtime verification tests within simulated conditions for dynamic analysis and automated scans with validation checks of the codebase for static analysis \cite[pp. 9--10]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}. According to Saavedra et al., incorporating intermediate representation (IR) and source-to-source translation into IaC static analysis facilitates the development of language-agnostic methods that can be ported to any IaC format \cite{saavedraPolyglotCodeSmell2023}. So, IT specialists emphasize that IaC codebases should be maintained in accordance with secure coding standards on a regular basis (e.g., ISO/IEC series, NIST frameworks, etc.) \cite{konalaFrameworkMeasuringQuality2025}. 

% subpart: visual IaC and other formats of information representation
Sandobalin et al. examined the advisability of using IaC modeling tools by training control groups on Argon and Ansible \cite{sandobalinEffectivenessToolsSupport2020}. The research showed that Argon, as a visual toolkit, is easier to learn and can abstract complexity as a thin layer above existing IaC configuration formats. Thus, allowing users to simply draw their IT architecture, while the underlying system automatically generates a specific configuration for deployment. It implies that complexity management and computational thinking also extend to graphical and other representations of information. Diagrams and visualizations provide two-dimensional or multidimensional representations, enabling more structured modes of reasoning, whereas text is inherently a linear representation of information. Certainly, these forms should not be regarded as mutually exclusive, rather they synergize with each other and are interchangeable at different levels of abstraction. 

% subpart: model driven development and specifications for AI
In this regard, Model Driven Engineering (MDE) methodology proposes treating documentation, specifications, and model definitions as primary and reference artifacts describing behavior and components of a system, from which code generation toolkits can facilitate transformation between them (e.g., into source code templates in any programming language or pseudocode) in order to achieve synchronization \cite{verbruggenPractitionersExperiencesModeldriven2023}. Firstly, it bridges the gap between developers and business-oriented specialists by demonstrating certain domain-specific problems and rules, so they can focus more on business logic. For instance, Alfonso et al. developed the BESSER platform based on MDE, which provides a canvas editor and multiple design notations like UML (Unified Modeling Language) for generating backend applications from diagrams \cite{alfonsoBuildingBESSEROpensource2024}. Such toolkits usually support the development of reliable prototypes and concepts, but it is not intended for high-load or performance-critical systems. Secondly, MDE allows structures to dynamically adapt to emerging requirements instead of manually rewriting boilerplate code and thinking about platform-specific implementations, so the development process is optimized. For example, it is applicable for automatic propagation of components changes and migration between layers instead of repetitive manual transformations, conversions, and modifications \cite{chillonPropagatingSchemaChanges}. Additionally, AI in the form of language models enables rapid code generation from specifications as detailed prompts and instructions \cite{diroccoUseLargeLanguage2025, stoicaSpecificationsMissingLink2024}. Although generative language models can cause challenges due to hallucinations and their stochastic nature, it might be partially mitigated through enforcing strict rules, validation mechanisms, and multiple series of output refinements \cite{shankarWhoValidatesValidators2024}.

\subsection{Resource management and accounting}
% @TODO: extract more valuable information from given references (sabettaKnownVulnerabilitiesOpen2024, SharedVisionSoftware, jaatunSoftwareBillMaterials2023) in order to extend this part

% subpart: IT resource accounting and dependencies in complex systems
OWASP (the Open Worldwide Application Security Project) identified 10 major risks for IT infrastructure which 5 of them are directly related to IT resources accounting \cite{OWASPWwwprojecttop10infrastructuresecurityrisks2025}: "Outdated Software", "Insufficient Threat Detection", "Insecure Resource and User Management", "Insecure Access to Resources and Management Components", "Insufficient Asset Management and Documentation". This fact clearly emphasizes that proper management of all system components and organizational knowledge is mandatory. In this context, software composition analysis (SCA) is a structured approach for collecting and accounting software components, defining common specifications and governance principles \cite{sabettaKnownVulnerabilitiesOpen2024}. It facilitated the establishment of a widely adopted format for software bill of materials (SBOM) with its multiple variations that allows to reduce a total vulnerability response time \cite{SharedVisionSoftware, jaatunSoftwareBillMaterials2023}. Other progressive solutions can assist in the same task by collecting data from a running production instance to subsequently generate recreatable configurations and identical images/snapshots \cite{kleinInfrastructureCodeFinal}. Usually, such procedures are combined with version control systems that store and track versions of all project files, allowing teams to have: process of collaborative development, a single source of truth, and well-defined compliance by localizing areas of responsibility \cite{ragavanVersionControlSystems2021}. Particularly, connections and links between system components form "dependencies" which have direct and indirect involvement on certain functionalities and qualities of base system. As demonstrated in Figure \ref{fig:components_of_complex_system}, OS (operating system) can be illustration of complex system requiring correct operation of its components providing emergent qualities.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_3}
    \caption{Components of a complex system.}
    \label{fig:components_of_complex_system}
\end{figure}

% subpart: third-party dependencies and example of ecosystems
According to statistics of the European Cyber Security Organisation (ECSO), solution-specific logic often represents only about 10\% or less of an application's total code, while the remainder is largely made up of third-party dependencies such as software (module, package, library), service (requests to external systems and network API), infrastructure (like drivers and underlying execution platform) \cite[p. 8]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}. Given a script in the Python programming language, the internal dependency in this case is the self-contained code, while the external dependency is everything that was created by third-party such as implementation of standard library, interpreter/compiler, specification, OS or platform-specific requirements, and so on, as shown on Figure \ref{fig:python_ecosystem_from_perspective_of_dependencies}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_4}
    \caption{Ecosystem of Python programming language from perspective of dependencies. Adapted from \cite[p. 8]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}.}
    \label{fig:python_ecosystem_from_perspective_of_dependencies}
\end{figure}

% subpart: service-oriented economy that formed global supply chain in IT
From a broader perspective, the modern economy is service-oriented and highly interconnected in the global supply chain, where everyone relies on each other, as represented in Figure \ref{fig:example_of_global_supply_chain}. Hence, engineers can mitigate issues of resource constraints (e.g., schedule, personnel, risks, requirements, finance, and qualifications) by integrating external dependencies and reusing code as outsourcing for common problems during the implementation and maintenance of IT products.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_5}
    \caption{Simplified example of the global supply chain in the IT context.}
    \label{fig:example_of_global_supply_chain}
\end{figure}

\subsection{Compromise and trust} 
% @TODO: edit this section and extend it with (algorithms, tables, figures) and translate to academic english
% @TODO: write more details to already created bullet points with detailed explanation
% @TODO: нужны примеры supply chain attack инцидентов в виде таблицы с перечислением (насколько увеличились в динамике и т.д.)
% @TODO: reference to constraint satisfaction problem, dependency theory, and similarity to game rules / game theory
% @TODO: reference for dependency management approaches (automatic and manual) with real examples
% @TODO: extract more details from given references
% @TODO: "aidenbergTrulyProtectVirtualizationBased2022" переписать что такой-то такой то автор приводит примеры обхода анализа and extract more details (такой же референс переиспользовать Results)

% subpart: supply chain attacks and dependency theory
On the other hand, any risks and failures in a single link can cause a cascading effect across the entire chain which is known as a supply chain attack. This attack vector exposes the ability to implicitly or explicitly influence on any IT infrastructure by leveraging direct and indirect (i.e., transitive) dependencies, invoking instability and fragility \cite{themitrecorporationSupplyChainCompromise2025}. It can be caused by any potential way of external influence, including unintentional flaws and defects, intentional threats like malicious software (malware), or destabilizing factors like breaking API changes and internal modifications \cite{okaforSoKAnalysisSoftware2022}. Inadequately governed dependency practices within a team can increase the system's exposure to software supply chain attacks. Accordingly, it is necessary to be familiar with various approaches to dependency management, such as:
\begin{itemize}
    \item Manual ("vendoring") - by copying an external dependency directly into the codebase in order to maintain and modify it manually.
    \item Automatic - dependency/package managers as seperate software system that is responsible for retrieving, handling, updating, and executing additional lifecycle operations on project dependencies.
\end{itemize}
These techniques might be combined together with build specification files (e.g., CMake, Make, Setuptools). Although some practicioners argue that the manual approach provides a more detailed understanding of dependency contents (source and distributions) and greater control (updating only explicitly), both of them are equally susceptible to supply chain attacks due to propagation of transitive dependencies. Consequently, dependencies that are not explicitly pinned should not be relied upon, since they may disappear or be replaced within the direct dependencies from which they originate. Adversaries actively exploit such kind of characteristics and behaviors. For example, someone can abuse identity confusion by impersonating as existing packages (via name similarity, description, or other metadata), claiming unregistered package names, and performing additional actions intended to mislead tools or users \cite{shradhaBeyondTypeSquatting291044}. Therefore, it is essential to understand the underlying mechanisms such as dependency resolution and their metadata parsing. From theoretical standpoint, dependency resolution is a variation on the topic of the constraint satisfaction problem where the objective is to satifsy a predefined set of conditions and rules. It is proven to be an NP-hard problem for which no optimal solution exists, meaning that constructing the dependency tree or graph becomes increasingly complex with each additional dependency and constraint (version requirements or other ecosystem-specific rules), as illustrated in Figure \ref{fig:dependency_theory}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_6}
    \caption{Dependency theory: (1) Version constraints that restrict the allowable range of package versions from the initial published version to the latest one according to the versioning format. Such versions basically represent a variation of the same entity. (2) Dependency groups (concept from set theory) which delimit or expand the number of required packages, because some components can be mandatory while others remain optional. (3) and (4) Nested dependency graph (concept from graph theory) with direct and transitive ones. In real projects, the number of dependencies and constraints is significantly larger, leading to dependency conflicts ("dependency hell") characterized by cross-references, circular or recursive relations, loops, duplicates, and similar pathological structures. These situations might be resolved, for example, by flattening the dependency tree into a more linear structure.}
    \label{fig:dependency_theory}
\end{figure}

\begin{comment}
% TODO: на самом деле алгоритм не ограничивывается этим и есть различные варианты to solve dependency resolution и представить в других формата и вариациях
% TODO: оформить как https://en.wikibooks.org/wiki/LaTeX/Algorithms
% TODO: pseudocode for resolving dependencies and backtracking, handle RecursionError
% TODO: references
- https://github.com/pypa/pip/tree/36987b0c31b97ffb9fb7949ded628e9a6b10c016/src/pip/_vendor/resolvelib
- https://pip.pypa.io/en/stable/topics/dependency-resolution/
- https://pip.pypa.io/en/stable/topics/more-dependency-resolution/
Выстраивание полного дерева зависимостей, разобрав работу “uv tree” и похожих инструментов для того чтобы понять как получить данные обо всех зависимостях (базовый алгоритм dependency resolving)
- устанавливаем определенную зависимость (e.g., matplotlib с указанием версии или условий этой версии)
- зависимость находится в переходном состоянии, пока не будет закреплена (“freezed”, “pinned”) и установлена
- для закрепления необходимо провести dependency resolution of dependency graph
    - найти все зависимости данной зависимости (прямые зависимости)
    - у каждой зависимости этой зависимости есть зависимости ниже и так далее (транзитивные зависимости)
    - но при такой рекурсивной проходке возникают dependency / circular dependency (когда одна зависимость зависит от другой или есть конфликт по определённым требованиям). Есть множество решений данной проблемы: дублировать скачиваемые зависимости но с другими версиями и условиями, и т.д.
    - К примеру, matplotlib имеет pillow в прямых зависимостях, но в свою очередь pillow имеет ещё множество зависимостей и так далее рекурсивно…
    - В идеале, хотелось бы дойти вглубь до кода который существует без зависимостей (внешних библиотек или установки), но это невозможно практически учитывая что есть stdlib python или libc которые тоже формально зависимости. Также надо учитывать, что есть зависимости которые не отмечены в pypi но они формально являются зависимостями (обертки над кодом C библиотек или кодов из другой экосистемы)
% TODO: упрощенный алгоритм парсинга dependencies (поиск всех файлов с requirements and всех зависимостей вниз но это не резолвинг сам по себе а лишь парсинг)
\end{comment}

% subpart: convenience/usability vs security (functional vs non-functional requirements)
% TODO: add reference about "distribution formats and categories"
Many software repositories prioritise functionality over security, often justifying this by claiming that they do not wish to restrict access for a community that is "open for everyone", mistakenly assuming that security represents an unnecessary overhead \cite{sasseDebunkingSecurityUsabilityTradeoff2016}. As a result, they expose themselves and others to the risk that anyone may maliciously influence ecosystems. So, users are left to manage the external chaos on their own. Moreover, existing market solutions cannot be regarded as definitive protections against supply chain attacks, as obtaining reliable data about software remains challenging due to its various distribution formats and categories:
\begin{itemize}
    \item white box - fully known content (e.g., open source);
    \item gray box - partially known content (e.g., leaked information, rumors, reverse engineering, insider knowledge, alternative products, or forks);
    \item black box - unknown content (e.g., functionality behind API like SaaS, proprietary and closed-source software).
\end{itemize}

% subpart: software composition analysis and other form of data gathering
For example, SCA tools typically focuse only on white box, gathering already indexed vulnerabilities or components, thereby leaving critical gaps in threat visibility and transparency. This is understandable, since other categories are inherently more complex due to specifics, because many companies use anti-analysis techniques to protect their software and intellectual property, making component-level inspection difficult or reasonably impossible \cite{zaidenbergTrulyProtectVirtualizationBased2022}. Despite rising SCA adoption by developers, multiple researchers indicate that supply chain vulnerabilities and incidents continue to increase, and the remediation process often remains slow \cite{senDeterminantsSoftwareVulnerability2020, jafariDependencyPracticesVulnerability2023}. Moreover, Alfadel et al. found that, in the Python ecosystem, it takes a median of up to 4 months to fix all occurrences of a security vulnerability in the PyPI repository \cite{alfadelEmpiricalAnalysisSecurity2021}. Experts also identify several additional drawbacks and limitations of existing SCA tools \cite{dietrichSecurityBlindSpots2023, enckTopFiveChallenges2022, bohmeSoftwareSecurityAnalysis2025}:
\begin{itemize}
    \item focusing more on static analysis rather than on dynamic analysis during runtime execution;
    \item they operate within the local environment instead of isolated or remote setups;
    \item comprehensive deep scanning of transitive dependencies is often missing;
    \item insufficient integration with AI and data analytics-based systems.
\end{itemize}

% subpart: changing the status quo
% @TODO: добавить референс на Bug Bounty систему также - платить комьюнити за поиск багов для "выстраивание системы модерации"
% @TODO: add reference for "secure by default", "call down", "mandatory oversight"
The fragmented software ecosystem worsens this situation, making accountability for the many packages and their adherence to standards unclear. Even software package that is trusted by the community cannot offer proper guarantees, because mainstream software licenses clearly mention that their products are distributed "as is" condition \cite{wintersgillLawDoesntWork2024}. Experts propose different solutions for the current status quo, ranging from weak to radical measures. Firstly, there should be an introduction of increased mandatory and strict basic requirements (baselines) at the platform level with compliance from end user side. This could reduce the number of attacks and malicious code due to high standards and improved quality. It also reflects the principle of "secure by default", where secure policies and good default configurations should become the common for many, and not just the exception reserved for professionals:
\begin{itemize}
    \item Mandatory oversight and analysis before package publication and subsequently with every update/change.
    \item Establishment of a moderation system where the community will naturally report and react to appearance of malicious packages.
    \item Policies for temporary blocking or issuing "call down" periods between user actions during anomalous activity (e.g., IP address change, password or specific token renewal, recent publications, and so on).
\end{itemize}

% subpart: regulations, hidden malware (Ken Thompson's hack), distinction between design and implementation, old model of trust (trusted/semi-trusted/untrusted)
% @TODO: add reference to https://arxiv.org/pdf/1803.07244 and to programming language model of execution (C++ abstract machine)
Secondly, the software market should pay more attention and show greater concern for formal regulatory mechanisms to increase trust and responsibility in the community, such as certification, auditing, and transparent processes, along with initiatives aimed at improving standards. Sammak et al. attended an interview among a small subset of experienced developers, which revealed that the majority of respondents are unfamiliar with details of software supply chain management or have a superficial understanding due to the absence of proper training on this topic for a broader audience \cite{sammakDevelopersApproachesSoftware2023}. Thirdly, popular or operation-critical software have a strong influence on whole landscape, so compromising them can undermine not just dependent infrastructure but also the global contracts of security, such as cryptography stack \cite{tsoupidiProtectingCryptographicLibraries2025}. Self-replicating malware or virus might compromise a significant portion of the IT ecosystem by ensuring that any code compiled on an infected hardware platform, compiler, or OS becomes compromised as well \cite{ladisaTaxonomyAttacksOpenSource2022}. This is due to the fact that definition files are merely instructions without guarantees that our intentions will be correctly expressed and transformed with preserved semantics in the produced output (e.g., machine code, process, and other representation) by intermediary system (compiler, interpreter, execution environment), as shown in Figure \ref{fig:design_and_implementation}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_7}
    \caption{Intermediary system between design and implementation.}
    \label{fig:design_and_implementation}
\end{figure}
Furthermore, conducting a compromise assessment or software evaluation for the produced product is complicated, because infected reverse engineering tools (e.g., debugger, decompiler, disassembler) or even their clean versions running in a compromised environment might hide the infected parts and backdoors as blind spots in the software in order to remain undetected. Due to the high stakes, it is crucial to determine whom to trust and based on what principles to minimize such risks beforehand. Researchers Singer and Bishop propose viewing trust in the form of a relationship model that makes us vulnerable because we rely and depend on a third party without any doubt due to assumptions and distractions, considering them trustworthy \cite{singerTrustBasedSecurityTrust2020}. They also suggest applying threat modeling and risk analysis before and during the design stage in the development lifecycle to minimize the factor of trust. Hence, trust should not be considered as a static and accurate indicator, but rather it is a dynamic probability of being trusted by someone else and vice 
versa, where any mistake from both parties leads to its loss. For example, one of the parties (recipient or sender) can at any time disrupt data transmission and its correctness by intentionally beginning not to adhere to the given standard protocols and interaction interfaces.

% subpart: zero trust as theory and in blockchain
FFor these reasons, the National Institute of Standards and Technology (NIST) developed the concept of Zero Trust Architecture (ZTA), in which trust requires continuous verification \cite{roseZeroTrustArchitecture2020}. Babenko et al. clearly demonstrated that empirical data on ZTA implementation correlates with a lower incident rate of insider threats \cite{babenkoCybersecuritylevelAssessmentModels}. Organizations can adopt this by following the key principles listed below:
\begin{itemize}
    \item Continuous evaluation of all actions and actors instead of granting full access within a protected perimeter. This means the environment itself does not affect how we trust someone/something, assuming operating in a hostile environment.
    \item Eliminating implicit trust in favor of explicit trust using a trust algorithm as a method for verifying access or credibility with transactional properties. % @TODO: cite zerotrust original document that page to "transactional properties"
    \item Segmentation of the network and systems into atomic cells, which theoretically fragments the attack surface (e.g., a large monolith will expose more data than individual microservices) \cite{JourneyZeroTrust}.
    \item Least privileges of access and minimizing amount of dependencies via strict comprehensive assessments of vendors or suppliers \cite{collierZeroTrustSupply2021}. % @TODO: add reference leeDoDEnterpriseDevSecOps where "collierZeroTrustSupply2021"
    \item Security as a dynamic process and adaptive system achieved through the continuous collection and analysis of data/statistics (resources, assets, incidents, etc.). % @TODO: также референс на explainable AI и контекст анализа данных в ZTA \cite{mushtaqSystematicLiteratureReview2025}, changing attack vectors (add reference pourmoafiSolutionSecuringInformation and mengCybersecuritySimondonsConcretization2022)
\end{itemize}
One example of ZTA implementation is blockchain systems, which contain zero-knowledge proofs and consensus algorithms such as "proof of something" that guarantees trust based on a defined trust factor (e.g., computational power, number of devices/actors, amount of specific tokens) \cite{banguiWhenTrustlessMeets2024}. Furthermore, there is a concept of "trustless" in the same sphere that futher reinforces ZTA and trust management approaches in distributed systems. Some studies even consider the potential for applying blockchain as a version control system due to its data integrity and non-repudiation characteristics \cite{grilliCombiningGitBlockchain2024}.

% subpart: secure by design and rethinking software ecosystems
Overall, the IT industry have to slow down and rethink what has already been produced, stopping the overproduction of software and beginning to "recycle waste" (old and abandoned ideas) by re-evaluating them, finding patterns, and recognizing repetition. This is due to the very high fragmentation of software solutions that compete with each other, but fundamentally they solve the exact same problem (e.g., frontend frameworks) \cite{enablingGlobalCollaboration}. Mirakhorli et al. reviewed 84 available SBOM management toolkits across multiple programming language ecosystems, which demonstrates the lack of unification and significant fragmentation in current solutions \cite{mirakhorliLandscapeStudyOpen2024}. Perhaps, developers should strive to regularly improve the standard library and make it better with "batteries included" in the official distribution. Otherwise, its position will be taken by many competing dependencies covering the exact same specific topic, thereby generating a high degree of entropy. Additionally, experts suggest designing systems in a way that they are "secure by design" in their foundation, where security and safety are built-in initially rather than being additional elements \begin{comment} @TODO: add reference "secure by demand" \end{comment}. It eliminates specific categories of vulnerabilities and forms a multilayered security / defense-in-depth approach \cite{mellMeasuringImprovingEffectiveness2016}. For instance, Berhe et al. showed the necessity for a mitigation plan and a regular budget on automated dependency recovery or manual actions such as replacing it with alternatives, upgrading it, or reverting to a previous release \cite{berheMaintenanceCostSoftware2023}. If there is considered well-decomposed systems that might still continue to function in such cases, or have recovery mechanisms ("self-healing") for the failure of individual components only up to a certain limit, after which the entire system becomes non-operational or some functionality is temporarily disabled \cite{dehrajReviewArchitectureModels2021}. IT community should strive for such next-generation systems that are comprehensive to the extent that they can self-analyze and self-improve, making them more reliable and less fragile.

% subpart: conclusion of literature review to understand research gap in current landscape
% @TODO: расширить эту часть подробнее как подведение итогов and identified research gaps
There is a practical problem with current industry approaches, requiring immediate innovations. While specifications are important, implementations of toolkits must also meet high-quality standards.

\begin{comment}
\section{Methodology}
% add table to literature review (сравнение с другими решениями) and results (анализ рисков).
% Добавить рефренс про сандбоксинг из NIST zero trust в начало методологию (где говориться про сандбоксы)
% привести приме как работают элементы других систем прежде чем раскрывать свою
% Показывает SOTA solutions раскрывая аспекты и характеристики, постепенно вычленяя определенные элементы которые слабо-развитые в его контексте - и постепенно тем самым переходит на методологии за счёт нахождения gaps и предлагает свои решения по этому повод (в конце лит обзора подводит итоги с переходом на методологию)
% TODO: Принцип работы современных с технической стороны
% add table to this part and literature review
% TODO: pseudocode в практической части для своей системы помимо блок-схем
% TODO: add about virtualization and digital twins (maybe to methodology briefly)
% TODO: Decompose to more subsections (depending on problem and components not just one large section). Методы (методология исследования) – методы, используемые для проверки гипотезы, должны быть описаны достаточно подробно, чтобы другой исследователь в этой области мог повторить проверку.
% Перенести все про виртуализацию и digital twins в методологию (concept explanation) типа будет мини лит обзор по этой теме, но в самой диссертации эту часть нужно подробно раскрыть ведь у виртуализации есть свои многие недостатки которые необходимо решить
% @TODO: refine methodology, results, conclusion
% comparative analysis, механизмы взаимодействия компонентов, portable/technology-agnostic
% They are also not an absolute solution, multiple underlying issues with sandbox also
% @TODO: expand this part and write more structured way
% Тут также добавить про то что системы будет открытой и любой сможет поучаствовать в развитии проекта в дальнейшем и его философия (что его отличает от конкурентов)
% добавить алгоритмы вместе с псевдокодом и UML diagrams и т.д. в методологию
The methodological part focuses on addressing problems and gaps identified in the literature review of the current landscape. A comprehensive lifecycle was developed, accompanied by essential architectural decisions.

\subsection{Concept and paradigm shift}
It was decided to explore principles from existing malware and binary analysis tools, such as sandboxes. These toolkits can effectively complement modern SCA approaches and enable a paradigm shift, as illustrated in Figure 6. The proposed idea involves redirecting the purpose of existing solutions toward a different target audience and analytical focus from regular end users to developers. Although this may appear to be a minor shift, it is both critical to have and straightforward to integrate.
% @TODO: need references and examples of such toolkits
% Тут доп. пояснение про задачу зависимостей (dependency theory and so on). И доп детали нормально что из себя представляет методология и т.д.
% Такой подход тоже не идеальный и у него есть свои limitations, но позволяет именно искать глубокие зависимости на самом деле а не просто просматривать поверхностно содержание
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{} % figure_6
    \caption{Paradigm shift by applying malware and binary analysis for SCA-related tasks} % TODO: also about simulations and digital twins
    \label{fig:placeholder}
\end{figure}
Sandbox and automated malware analysis enable examination of software within isolated, reproducible execution environments. They also support custom scripted scenarios that emulate developer behavior when interacting with specific libraries or applications (e.g. according to use cases or user stories), thereby revealing hidden during runtime. These tools offer a rich set of functionalities and methodologies that are highly relevant and should be leveraged for advancing software dependency analysis.

\subsection{System modeling}
The problem domain was decomposed into several subtopics that naturally align with required components and services, ensuring that it remains both extensible and customizable. The architectural design was based on the principles of hexagonal architecture, because it provides a clear separation between supplementary elements and the core business logic, such as driving side, application, and driven side. The model introduces the concept of adapters and ports, borrowed from electronic engineering, which enables the replacement or modification of component implementations. Firstly, the driving side represents the system’s entry points, including human users and external automated systems that interact with the platform through backend APIs and web interfaces over network connections within client-server model, as clearly shown in Figure 7.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_7}
    \caption{Driving side of the architecture}
    \label{fig:placeholder}
\end{figure}

Secondly, the internal part of the application is designed as a modular monolith, divided into multiple components or services, as illustrated in Figure 7:
\begin{itemize}
    \item “Identity provider, user and session management” – maintains all information about users in the platform with their metadata. Generally, “Authentication” and “Authorization” operate alongside with that, forming a comprehensive set of components responsible for user-related security operations: roles or access control based on permissions, login and register handling, access/refresh token management, checking behavior and actions of user, multi-factor authentication (MFA), and so on.
    \item “Groups and project system” – holds multiple analyses in which users can collaborate with each other.
    \item “Moderation system” and “Administration system” – are intended for platform operators, enabling internal system management and response to user requests as needed.
    \item “Analysis system” – is a critical component that performs all essential operations required for software analysis.
\end{itemize}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_8}
    \caption{Internal application components of the architecture}
    \label{fig:placeholder}
\end{figure}

Components can be added, modified, or disabled as needed. This flexibility improves security by reducing the attack surface, minimizing system states, decreasing computational complexity, and factor of combinatorial explosion. The analysis engine also features a plugin and modding system with multiple operating modes. This means that users of the system can develop their own plugins for specific use cases by implementing the provided interface. Such extensibility enables the analysis not only of software packages but also of container images, computer networks, and other parts of IT infrastructure. Furthermore, there is support for fine-grained configuration and operation modes. For example, specifying which data to collect or ignore, potentially through regex-based filtering.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_9} % Redraw as UML Class diagram short with bundle example for plugin system
    \caption{Modification of system}
    \label{fig:placeholder}
\end{figure}

The analysis engine consists of tightly coupled contextual components, all operating around a common abstraction of “task”:
\begin{itemize}
    \item Orchestrator — is responsible for task queuing, scheduling, and overall management operations. It serves as an intermediate layer that receives tasks directly from the analysis system, each with specific requirements that must be resolved before delegation to other components. At the same time, it manages and coordinates the overall execution process, acting as the central communication and control hub, because components in the Bundle cannot begin execution without confirmation or explicit request from Orchestrator. In this context, Orchestrator interacts with the Bundle abstraction rather than directly with its individual components, which contains a set of predefined task executors as described below. Also, the connection between Orchestrator and Bundle follows a one-to-many relationship, where Orchestrator determines which Bundle to use among several available ones.
    \item Instancer (i.e., Provisioner) – creates, prepares, and manages the required environments (e.g., containers, VMs, specific OS or images, etc.).
    \item Collector (i.e., Extractor) – continuously or periodically collects data from created instance (e.g., logs, files, making image screenshots or recording videos, memory or disk snapshots, etc.).
    \item Interactor – performs predefined set of actions or scripted scenarios in the isolated environment to simulate user behavior and trigger malware responses (e.g., interaction with file system, launching or installing programs, managing input/output devices, etc.).
    \item Parser – transforms the required data into a unified format, such as an intermediate representation (IR), or converts it into a format suitable for Analyzer.
    \item Analyzer – produces specific verdicts or predictions based on the received data (e.g., determining critical components within dependency tree, performing other specialized analyses using both stochastic and deterministic algorithms).
\end{itemize}

Thirdly, Figure 8 illustrates the components of the driven side along with some generic infrastructure elements that are required for proper application functionality. % @TODO: описать за что каждый компонент из driven side отвечает и зачем он там
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{}
    \caption{Driven side of the architecture}
    \label{fig:placeholder}
\end{figure}

Overall, the prototype demonstrates a variation that can be implemented independently of specific technologies, making it a technology-agnostic and portable solution. % вместо завершения дальше продолжить и представить алгоритмы

\section{Results}
% TODO: Результаты – гипотеза должна быть проверена, а данные, представляющие результаты проверки, представлены. (обязательно графики и таблицы с результатами данных). результат это алгоритм, метод, модель, архитектура для статьи и также обоснование
The results include an analysis of security and critical decisions derived from the proposed methodology. %  as well as a comparison with existing market solutions addressing the same topic.
\subsection{Threat modeling and risks}
% TODO: Добавить про потенциальные риски также
The analysis emphasized the external perimeter while also exploring specific elements of the PyPI platform, which already demonstrates several comparable and thematically relevant aspects. It hosts a large number of different Python packages and is owned by the Python Software Foundation (PSF) which operates under a free and open model, where any user may use the package manager or other mechanisms to upload, download, and publish packages in the form of source or build distributions. Similar systems and policies are followed by other software repositories and ecosystems, such as Maven Central, NPM, Crates, and many others. 
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{}
    \caption{DFD legend}
    \label{fig:placeholder}
\end{figure}
Also, there was prepared the DFD (data-flow diagram) legend with all necessary elements for the drawing process, as represented in Figure 9. Moreover, constructed level 0 DFD as shown in Figure 10. It was consciously decided to omit aspects concerning connectivity to the internet (e.g., internet service providers and intermediary network components), as those elements fall outside the scope of this analysis.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{} % span across 2 columns wide at the start of page
    \caption{Level-1 DFD (interaction interfaces with the platform)}
    \label{fig:placeholder}
\end{figure}

\subsection{Tradeoffs in the architecture prototype}
This section outlines the critical architectural decisions made and the reasoning behind them. Firstly, it was decided to keep the Parser and Analyzer separate to ensure better decomposition; however, merging them into a single entity could improve performance and efficiency. Secondly, to reduce the load and prevent bottlenecks on a single Orchestrator, multiple instances of the Engine should be deployed, or the system should run in a concurrent or parallel mode. Thirdly, it was decided to group task executors into a single Bundle, as these components were found to be highly interdependent. Initially, they were considered as separate entities without direct communication, following a microkernel architecture in which component interaction occurs asynchronously (via message bus and inter-process communication). However, this approach proved unsuitable, since Collector depends on the specific implementation of the data source, Parser requires awareness of data format, and Analyzer cannot detect anomalies or draw conclusions from incompatible input data.

\section{Conclusion} % Заключение – данные должны быть обсуждены, результаты интерпретированы и сделаны выводы.
The topic of supply chain management in IT infrastructure remains highly relevant, as the number of related incidents continues to grow despite the emergence of new tools and standards. More fundamental, preventive measures may be required at the source level, where software packages are produced, rather than reacting after incidents occur.

Future research will focus on converting the proposed methodology into a microservice-based system, which remains a challenging task due to its complexity and management requirements. However, such an approach would enable scalability and continuous integration of new functionality without interrupting the platform. Further work will also include a full implementation of the system and an independent statistical analysis of software dependencies beyond those examined in the current literature review.
\end{comment}

% \nocite{*}
\printbibliography % deduplication and autoformatting https://tex.stackexchange.com/questions/233086/avoiding-duplicate-entries-in-bibliography-having-different-cite-keys, Also check if rendering is right for IEEETran and links also represented https://libraryguides.vu.edu.au/ieeereferencing/technicalreports

\end{document}
