% General styles and packages:
\documentclass[conference, comsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{listings}
\usepackage[none]{hyphenat} % turn off hyphenation
\setlength{\parskip}{0pt}   % no vertical space between paragraphs
\graphicspath{{figures/}}   % default path for figures

% Code listing styles:
\lstdefinestyle{code_style}{ 
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    language=Python,
    frame=single,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=code_style}

% References (IEEE bibliography styles and format for biblatex):
\usepackage[backend=biber, style=ieee, sorting=none, url=false]{biblatex}
\addbibresource{references.bib}
\renewcommand*{\bibfont}{\footnotesize}
\setlength{\biblabelsep}{\labelsep}
\setlength{\bibitemsep}{\IEEEbibitemsep}

% Heading:
\makeatletter
\def\ps@IEEEtitlepagestyle{
  \def\@oddhead{\parbox{\textwidth}{
    {\footnotesize
    2026 IEEE 6th International Conference on Smart Information Systems and Technologies (SIST)\\
    13--15 May 2025, Astana, Kazakhstan\par}
  }}
  \def\@oddfoot{}
}
\makeatother

\begin{document}
\title{Customizable methodology for in-depth analysis of software within isolated computational environments to secure critical IT infrastructures\\}

\author{\IEEEauthorblockN{1\textsuperscript{st} Noyan Tendikov}
\IEEEauthorblockA{\textit{School of Cybersecurity} \\
\textit{Astana IT University}\\
Astana, Kazakhstan \\
242710@astanait.edu.kz \\
ORCID: 0009-0009-7251-8830}
}
\author{\IEEEauthorblockN{2\textsuperscript{nd} Leila Rzayeva}
\IEEEauthorblockA{\textit{School of Cybersecurity} \\
\textit{Astana IT University}\\
Astana, Kazakhstan \\
l.rzayeva@astanait.edu.kz \\
ORCID: 0000-0002-3382-4685}
}
\author{\IEEEauthorblockN{3\textsuperscript{rd} Rostyslav Lisnevskyi}
\IEEEauthorblockA{\textit{School of Cybersecurity} \\
\textit{Astana IT University}\\
Astana, Kazakhstan \\
rostyslav.lisnevskyi@astanait.edu.kz \\
ORCID: 0000-0002-9006-6366}
}

\maketitle

\begin{abstract}
Critical IT infrastructures are increasingly threatened by hidden attacks targeting their components. Malicious actors can infiltrate silently through software dependencies by launching an indirect attack on internal vulnerabilities. This research aims to explore this issue by focusing on software composition and its unification, extending the current approaches through a prototype framework for analyzing software in specialized isolated environments. The preliminary results of risk assessment and threat modeling demonstrate its viability and security properties. Further refinement and studies are required, including a reference implementation, a comprehensive technical specification, and iterative field research with detailed statistics based on the implemented approach.
\end{abstract}

\begin{IEEEkeywords}
software supply chain; infrastructure-as-code; critical IT infrastructure; software composition analysis; software dependencies; software bill of materials; sandboxing; malware and binary analysis; digital twins; intelligent agents
\end{IEEEkeywords}

\section{Introduction}
% subpart: current sutation about escalation of conflicts that affect on cybersecurity of critical infrastructures
Modern cyber conflicts have escalated into a condition of constant cyberwar, driven by countless threats. Unlike traditional military conflicts, this warfare is taking place within a digital space and often goes unnoticed by the general public due to their lack of awareness and competence \cite{almeidaComparativeAnalysisEUbased2025}. The current policy of major states is dictated by the principle of "peace through strength", prioritizing the rule of power over ethical considerations and international law. It leads to an intensified arms race between multiple actors, such as government agencies, armies, corporations, and private groups. Under these circumstances of ongoing confrontations between defenders and attackers, ensuring security is only a continuous process rather than an absolute guarantee \cite{safitraCounterattackingCyberThreats2023}. So, the industry of information technology (IT) should be engaged in systematic risk assessment and apply reactive patches as problems emerge, because the number of potential threats and vulnerabilities is inherently unknown and cannot be fully identified or eliminated in upfront. Moreover, the increasing rate of automation and digitalization exposes associated risks for critical infrastructure, where their compromise could create dangerous situations and collateral damage for citizens \cite[pp. 1--2]{nationalinstituteofstandardsandtechnologyFrameworkImprovingCritical2018}. For example, the Cybersecurity and Infrastructure Security Agency (CISA) of the USA adopted a classification that identifies 16 critical infrastructure sectors \cite[pp. 6--7]{InfrastructureResiliencePlanning}, including the context of IT infrastructures. This sector is composed from corresponding services and assets (e.g., maintenance personnel, electricity, networks, equipment, hardware, software), providing an operational basis or computational environment to utilize for information systems (IS) \cite{qatawnehMediatingRoleTechnological2024}.

% subpart: reasons and factors of attacks on critical infrastructure and their consequences
Fundamentally, attacks targeting critical IT infrastructure are driven by diverse motives and reasons, overlapping with other industries \cite{lehtoCyberattacksCriticalInfrastructure2022}. Historical events have already shown that conflicts of national interests might trigger a security breach on highly protected facilities (e.g., nuclear power plants) \cite{makrakisIndustrialCriticalInfrastructure2021}. For instance, Riggs et al. examined statistics on global cyberattacks against critical infrastructures since 2006 and developed a prediction model that shows exponential growth of incidents with the military sector being the most frequently targeted \cite{riggsImpactVulnerabilitiesMitigation2023}. Another factor that may escalate the situation is the growing trend around generative artificial intelligence (AI), which increases the demand on data centers, stimulating competitors and other third parties to compromise or abuse that computational power for their own benefit \cite{yigitCriticalInfrastructureProtection2024}. Additionally, the increasing interconnectivity of IS, especially through Internet of Things (IoT) devices, causes fragility by exposing multiple points of failure and allows malicious actors or even device manufacturers to gain remote access to them \cite{vardakisReviewSmarthomeSecurity2024, rauhalaPhysicalWeaponizationSmartphone2022}.

% subpart: research goal and structure of the study (scope of the work, new advances, hypothesis, research questions, theories and/or applications)
Therefore, this study attempts to construct security measures for the IT infrastructure, emphasizing the process of collecting and accounting of its internal resources or components for reliability and early insider threat prevention. Firstly, the literature review covers the historical development of IT infrastructure, complexity management, and issues with software composition addressing current needs and challenges in the field. Secondly, the methodology specifies the hypotheses and provides a framework for conducting customizable analyses within sandboxed environments. Thirdly, the results part examines features of the proposed solution. Lastly, the conclusion summarizes findings and potential drafts for future work.

\section{Literature review}
\subsection{Historical development of IT infrastructure}
% subpart: commercialization of IT
The computing ecosystem and its service models have evolved gradually to on-premises and cloud paradigms, becoming increasingly commercialized to meet customer demands and scaling issues, as broadly represented in Figure \ref{fig:developmental_milestones_of_computing_environment}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{developmental_milestones_of_computing_environment}
    \caption{Developmental milestones of computing environments (VM - virtual machine; C - container; PaaS, IaaS, SaaS - external services): (1) Mainframe. (2) Cluster of servers. (3) Geographically distributed servers with virtualization (visually represented by color difference). (4) - Hybrid approach with cloud services.}
    \label{fig:developmental_milestones_of_computing_environment}
\end{figure}

% subpart: growing ecosystem of IT infrastructure
Initially, there were only mainframes that required dedicated teams for maintenance and performed at a highly mechanical level, so their bulkiness and slow speed enforced considerations for the adaptation of distributed computations \cite[Sec. 3.1]{lindsayEvolutionDistributedComputing2021}. Fortunately, improvements in IT manufacturing significantly contributed to emergence of dedicated servers that could be networked together in a modular way to provide scalability. Following this trend, virtualization moved beyond bare-metal deployments by allowing isolation layers to securely run multiple virtual machines (VMs) on shared hardware to significantly enhance efficiency of resource utilization \cite{randalIdealRealRevisiting2021}. Eventually, increasing demand for international connectivity and economic globalization naturally formed large-scale data centers with network of geographically distributed servers in order to optimize content delivery and processing \cite[pp. 9--11]{lindsayEvolutionDistributedComputing2021}. 

% subpart: "as-a-service" models and security implications 
Nowadays, organizations can offload many responsibilities and parts of their IT infrastructure to cloud and service providers which abstract physical machines and offer instant on-demand resource provisioning and management, thereby reducing operational burdens via "as-a-service" models with specialized offerings: software as a service (SaaS), platform as a service (PaaS), infrastructure as a service (IaaS) \cite{mellNISTDefinitionCloud2011}. Software engineers often take that for granted, underestimating interdependence of software and hardware sides for facilitating the execution of general-purpose tasks within multiple layers of abstraction. Such service-based approach for outsourcing still requires integrating and coordinating operations over delegated parts (e.g., security policies, privacy compliance, and external monitoring). Moreover, certain strategic systems (i.e., governmental and military) and their data are prohibited from being hosted in the cloud due to secrecy laws and regional restrictions \cite{arifComprehensiveSurveyPrivacyEnhancing2025, alghofailiSecureCloudInfrastructure2021}.

\subsection{Complexity management with codification and modeling}
% subpart: codification that tries to solve complexity and constructed IaC
As the sector of IT infrastructure has expanded, its complexity and associated challenges have emerged accordingly. Nonetheless, this complexity has become the primary factor for the process automation against manual, repetitive, and impractical operations. Therefore, automation and orchestration are no longer optional - they are critical for leveraging fully scalable and reliable systems from a strategic perspective. One of the natural strategies for complexity management is code. The process of codification captures procedures, routines, or algorithms of actions in a textual format \cite[pp. 1--2]{ouriquesRoleKnowledgebasedResources2023}. For example, Chuprikov et al. propose specifying security policies in IT systems in a form of domain specific language (DSL) that can be ran within an execution engine \cite{chuprikovSecurityPolicyCode2025}. Codification also established the paradigm of Infrastructure-as-Code (IaC) which is focused on specifying the state and content of IT infrastructures to enable reproducibility (e.g., provisioning, replication, copying, scaling) and modification (with emphasis on transparency and maintainability), as well as management and maintenance by self-documenting code or files that incorporate defined metadata, configurations, and procedures \cite{pahlInfrastructureCodeTechnology2025}. It minimizes human errors and establishes immutable infrastructure with the phoenix model, allowing any server instance to be recreated, replaced, updated, or destroyed from code without losing critical configurations. This approach eliminates the existence of snowflake servers, which are deviated systems with configuration drift and unpredictability due to their manual modification and absence of documentation \cite[Ch. 2]{morrisInfrastructureCodeDynamic2020}.

% subpart: how IaC tools operate
IaC operations are managed by system administrators, DevOps teams (Development and Operations), SREs (Site Reliability Engineers), or IT professionals via IaC toolkits (e.g., Terraform, Ansible), enforcing the utilization of definition files (e.g., templates, playbooks, manifests), configurations, or execution steps for servers, networks, databases, installations, updates, backups, and so on \cite[Ch. 4]{morrisInfrastructureCodeDynamic2020}. They might be implemented internally via general-purpose languages, but using configurations as application programming interfaces (APIs). For example, containerization tools like Docker use Dockerfiles to define application dependencies and versions, while the runtime builds a container with artifacts according to specified requirements, as shown in Figure \ref{fig:iac_as_self_documenting_system}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{iac_as_self_documenting_system}
    \caption{Processes of applying changes to servers: (1) Manually. (2) Via IaC pipeline as a self-documenting system.}
    \label{fig:iac_as_self_documenting_system}
\end{figure}

% subpart: secure coding practices for IaC configurations, scripts, and code
Any IaC file should be proceeded through the continuous integration and continuous delivery (CI/CD) pipeline, which maintains safe deployment by runtime verification tests within simulated conditions for dynamic analysis and automated scans with validation checks of the codebase for static analysis \cite[pp. 9--10]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}. According to Saavedra et al., incorporating intermediate representation (IR) and source-to-source translation into IaC static analysis facilitates the development of language-agnostic methods that can be ported to any IaC format \cite{saavedraPolyglotCodeSmell2023}. So, IT specialists emphasize that IaC codebases should be maintained in accordance with secure coding standards on a regular basis (e.g., ISO/IEC series, NIST frameworks, etc.) \cite{konalaFrameworkMeasuringQuality2025}.

% subpart: visual IaC and other formats of information representation
Sandobalin et al. examined the advisability of using IaC modeling tools by training control groups on Argon and Ansible \cite{sandobalinEffectivenessToolsSupport2020}. The research showed that Argon, as a visual toolkit, is easier to learn and can abstract complexity as a thin layer above existing IaC configuration formats. Thus, allowing users to simply draw their IT architecture, while the underlying system automatically generates a specific configuration for deployment. It implies that complexity management and computational thinking also extend to graphical and other representations of information. Diagrams and visualizations provide two-dimensional or multidimensional representations, enabling more structured modes of reasoning, whereas text is inherently a linear representation of information. Certainly, these forms should not be regarded as mutually exclusive, rather they synergize with each other and are interchangeable at different levels of abstraction. 

% subpart: model driven development and specifications for AI
In this regard, Model Driven Engineering (MDE) methodology proposes treating documentation, specifications, and model definitions as primary and reference artifacts describing behavior and components of a system, from which code generation toolkits can facilitate transformation between them (e.g., into source code templates in any programming language or pseudocode) in order to achieve synchronization \cite{verbruggenPractitionersExperiencesModeldriven2023}. Firstly, it bridges the gap between developers and business-oriented specialists by demonstrating certain domain-specific problems and rules, so they can focus more on business logic. For instance, Alfonso et al. developed the BESSER platform based on MDE, which provides a canvas editor and multiple design notations like UML (Unified Modeling Language) for generating backend applications from diagrams \cite{alfonsoBuildingBESSEROpensource2024}. Such toolkits usually support the development of reliable prototypes and concepts, but it is not intended for high-load or performance-critical systems. Secondly, MDE allows structures to dynamically adapt to emerging requirements instead of manually rewriting boilerplate code and thinking about platform-specific implementations, so the development process is optimized. For example, it is applicable for automatic propagation of components changes and migration between layers instead of repetitive manual transformations, conversions, and modifications \cite{chillonPropagatingSchemaChanges}. Additionally, AI in the form of language models enables rapid code generation from specifications as detailed prompts and instructions \cite{diroccoUseLargeLanguage2025, stoicaSpecificationsMissingLink2024}. Although generative language models can cause challenges due to hallucinations and their stochastic nature, it might be partially mitigated through enforcing strict rules, validation mechanisms, and multiple series of output refinements \cite{shankarWhoValidatesValidators2024}.

\subsection{Resource management and accounting}
% subpart: IT resource accounting and dependencies in complex systems
% @TODO: extract more valuable information from given references (sabettaKnownVulnerabilitiesOpen2024, SharedVisionSoftware, jaatunSoftwareBillMaterials2023) in order to extend this part. And more about owasp cyclonedx, SLSA, and related ecosystem of SCA tools.
OWASP (the Open Worldwide Application Security Project) identified 10 major risks for IT infrastructure which 5 of them are directly related to IT resources accounting \cite{OWASPWwwprojecttop10infrastructuresecurityrisks2025}: "Outdated Software", "Insufficient Threat Detection", "Insecure Resource and User Management", "Insecure Access to Resources and Management Components", "Insufficient Asset Management and Documentation". This fact clearly emphasizes that proper management of all system components and organizational knowledge is mandatory. In this context, there is already a structured approach for collecting and accounting software components, such as software composition analysis (SCA), which defines common specifications and governance principles \cite{sabettaKnownVulnerabilitiesOpen2024}. It facilitated the establishment of a widely adopted format for software bill of materials (SBOM) with its multiple variations that allows to reduce a total vulnerability response time \cite{SharedVisionSoftware, jaatunSoftwareBillMaterials2023}. Other progressive solutions can assist in the same task by collecting data from a running production instance to subsequently generate recreatable configurations and identical images/snapshots \cite[pp. 5--7]{kleinInfrastructureCodeFinal}. Usually, such procedures are combined with version control systems (VCS) that store and track versions of all project files, allowing teams to have: process of collaborative development, a single source of truth, and well-defined compliance by localizing areas of responsibility \cite{ragavanVersionControlSystems2021}. Particularly, connections and links between system components form "dependencies" which have direct and indirect involvement on certain functionalities and qualities of base system. As demonstrated in Figure \ref{fig:components_of_abstract_complex_system}, OS (operating system) can be illustration of complex system that requires correct operation of its components providing emergent qualities.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{components_of_abstract_complex_system}
    \caption{An abstract complex system that is consisted of multiple underlying operational components.}
    \label{fig:components_of_abstract_complex_system}
\end{figure}

% subpart: third-party dependencies and example of ecosystems
According to statistics of the European Cyber Security Organisation (ECSO), solution-specific logic often represents only about 10\% or less of an application's total code, while the remainder is largely made up of third-party dependencies such as software (module, package, library), service (requests to external systems and network API), infrastructure (like drivers and execution platform) \cite[p. 8]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}. Given a script in the Python programming language, the internal dependency in this case is the self-contained code, while the external dependency is everything that was created by third-party such as implementation of standard library, interpreter/compiler, specification, OS or platform-specific requirements, and so on, as shown on Figure \ref{fig:python_ecosystem_from_perspective_of_dependencies}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{python_ecosystem_from_perspective_of_dependencies}
    \caption{Ecosystem of Python programming language from the perspective of dependencies. Adapted from \cite[p. 8]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}.}
    \label{fig:python_ecosystem_from_perspective_of_dependencies}
\end{figure}

% subpart: service-oriented economy that formed global supply chain in IT
% @TODO: add references about global supply chain and how it is generally different in software side
From a broader perspective, the modern economy is service-oriented and highly interconnected in the global supply chain, where everyone relies on each other, as represented in Figure \ref{fig:simplified_example_of_global_supply_chain}. Hence, engineers can mitigate issues of resource constraints (e.g., schedule, personnel, risks, requirements, finance, and qualifications) by integrating external dependencies and reusing code as outsourcing for common problems during the implementation and maintenance of IT products.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{simplified_example_of_global_supply_chain}
    \caption{Simplified example of the global supply chain in the IT context.}
    \label{fig:simplified_example_of_global_supply_chain}
\end{figure}

\subsection{Compromise and trust in security.}
% subpart: supply chain attacks and dependency theory
% @TODO: add references to constraint satisfaction problem, dependency theory, and its similarity to game rules / game theory.
% @TODO: add reference for dependency management approaches (automatic and manual) with real world examples.
% @TODO: examples of major supply chain attack incidents in table and linear graph with amount of them to understand dynamics of change. Even unusual attack vectors of supply chain attacks and malware via game mods (roblox, gmod, minecraft).
On the other hand, any risks and failures in a single link can cause a cascading effect across the entire chain which is known as a supply chain attack. This attack vector exposes the ability to implicitly or explicitly influence on any IT infrastructure by leveraging direct and indirect (i.e., transitive) dependencies, invoking instability and fragility \cite{themitrecorporationSupplyChainCompromise2025}. It can be caused by any potential way of external influence, including unintentional flaws and defects, intentional threats like malicious software (malware), or destabilizing factors like breaking API changes and internal modifications \cite{okaforSoKAnalysisSoftware2022}. Inadequately governed dependency practices within a team can increase the system's exposure to software supply chain attacks. Accordingly, it is necessary to be familiar with various approaches to dependency management, such as:
\begin{itemize}
    \item Manual ("vendoring") - by copying an external dependency directly into the codebase in order to maintain and modify it independently.
    \item Automatic - dependency/package managers as seperate software system that is responsible for retrieving, handling, updating, and executing additional lifecycle operations on project dependencies.
\end{itemize}
These techniques might be combined together with build specification files (e.g., CMake, Make, Setuptools). Although some practicioners argue that the manual approach provides a more detailed understanding of dependency contents (source and distributions) and greater control (updating only explicitly), both of them are equally susceptible to supply chain attacks due to propagation of transitive dependencies. Consequently, dependencies that are not explicitly pinned should not be relied upon, since they may disappear or be replaced within the direct dependencies from which they originate. Adversaries actively exploit such kind of characteristics and behaviors. For example, someone can abuse identity confusion by impersonating as existing packages (via name similarity, description, or other metadata), claiming unregistered package names, and performing additional actions intended to mislead tools or users \cite{shradhaBeyondTypeSquatting291044}. Therefore, it is essential to understand the underlying mechanisms such as dependency resolution and their metadata parsing. From theoretical standpoint, dependency resolution is a variation on the topic of the constraint satisfaction problem where the objective is to satisfy a predefined set of conditions and rules. It is proven to be an NP-hard problem for which no optimal solution exists, meaning that constructing the dependency tree or graph becomes increasingly complex with each additional dependency and constraint (version requirements or other ecosystem-specific rules), as illustrated in Figure \ref{fig:brief_introduction_to_dependency_theory}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{brief_introduction_to_dependency_theory}
    \caption{A brief introduction to dependency theory: (1) Version constraints that restrict the allowable range of package versions from the initial published version to the latest one according to the versioning format. Such versions basically represent a variation of the same entity. (2) Dependency groups (concept from set theory) which delimit or expand the number of required packages, because some components can be mandatory while others remain optional. (3) and (4) Nested dependency graph (concept from graph theory) with direct and transitive ones ("1:M" means "one to many" relationships). In real projects, the number of dependencies and constraints is significantly larger, leading to dependency conflicts ("dependency hell") characterized by cross-references, circular or recursive relations, loops, duplicates, and similar pathological structures. These situations might be resolved, for example, by flattening the dependency tree into a more linear structure.}
    \label{fig:brief_introduction_to_dependency_theory}
\end{figure}

\begin{comment}
% @TODO: some bug in pdf due to this comment
@TODO: Add in-text reference to fig:dependency_resolution to text about right after fig:dependency_theory. For example, "One of the well-known or widely applied dependency resolution algorithm is backtracking, which is indicated in Figure \ref{dependency_resolution}".
@TODO: Add visual line breaks and wrapping, so code listing is not outside of column boundary.
@TODO: Convert to abstract algorithm instead of Python code by using "algorithmic" package instead of "listing" (https://tex.stackexchange.com/questions/128723/algorithm-as-figure-and-without-italic-and-bold-formatting).
@TODO: Seperate algorithms to subfigures: backtracking algorithm and dependency resolution with parsing.
@TODO: Add doctests and unittests to all code listings as form of literate programming and examples how code really works.
@TODO: Edit algorithms in figure "dependency resolution with parsing" according to provided references to be correct with reality:
- https://github.com/pypa/pip/tree/36987b0c31b97ffb9fb7949ded628e9a6b10c016/src/pip/_vendor/resolvelib
- https://pip.pypa.io/en/stable/topics/dependency-resolution/
- https://pip.pypa.io/en/stable/topics/more-dependency-resolution/
- https://docs.astral.sh/uv/reference/internals/resolver/
- https://docs.astral.sh/uv/concepts/resolution/
@TODO: Add all steps of dependency management pipeline to pseudocode listing:
1. Set some dependency (e.g., matplotlib with version specifier and other requirements).
2. Dependency is under "transition state" if it is not pinned ("freezed") and downloaded.
3. In order to pin exact dependency, we have to do dependency resolution of dependency graph:
    - Step 1. Parse already specified direct dependencies in requirements.txt file.
    - Step 2. Find all dependencies of direct dependencies (transitive dependencies from direct dependencies).
    - Step 3. Continue Step 2. in recursive way if there is no errors and dependency conflicts/hell (e.g., circular dependency).
    - Note 1. Issues in Step 3. can be mitigated by different approaches: duplicating downloading dependencies but with other versions and requirements (e.g., like npm does).
    - Note 2. It is worth to consider that there are dependencies that is not mentioned in PyPI because they are internal components of some wrappers and belongs to other ecosystems (e.g., wrapper around C library). Therefore, we should go deeper and identify all possible dependencies until non-dependent code/components to "solid foundation" (basic libc, os-specific dependencies, drivers, stdlib in programming language ecosystem, etc.).
\begin{figure}[htbp]
    \centering
    \begin{lstlisting}
from typing import TypedDict

class Constraint(TypedDict):
    version: str # need to be parsed
    ... # can be other constrains and rules
    # (e.g., everything except specific package name and date)

class Package(TypedDict):
    name: str # or identity in any other form (e.g., unique id)
    constraints: List[Constraint]

def resolve_dependencies(packages: list[Package]) -> Value | Error:
    """Resolve dependencies in software packages"""
    # Value - resolved tree/graph

    # TODO: states and assumptions (from previous call)
    # TODO: handle RecursionError
    # TOOD: merge Constraint and Package to Requirement (or not?)
    if not packages:
        raise ValueError("should be at least one direct package")
    if not isinstance(packages, list):
        raise TypeError("should be a list type")
    for package in packages: # package by package (one by one)
        if not package.constrains: # is empty
            # giving default assumption (latest)
        # ...
        # for constraint in package.constraints:
            # try to satisfy it

# Other variant to represent it:
def resolve_dependencies(packages: List[Package]) -> ResolvedTree | Error:
    where :...

    do_backtracking():
        assumption = change_according_to_last_call()
        resolve_dependencies(...)

    \end{lstlisting}
    \caption{Dependency resolution and parsing.}
    \label{fig:dependency_resolution}
\end{figure}
\end{comment}

% subpart: convenience/usability vs security (functional vs non-functional requirements)
% @TODO: proofs that "functionality over security" is wrong or inconsistent
Many software repositories prioritise functionality over security, often justifying this by claiming that they do not wish to restrict access for a community that is "open for everyone", mistakenly assuming that security represents an unnecessary overhead \cite{sasseDebunkingSecurityUsabilityTradeoff2016}. As a result, they expose themselves and others to the risk that anyone may maliciously influence ecosystems. So, users are left to manage the external chaos on their own. Despite rising SCA adoption by developers, multiple researchers indicate that supply chain vulnerabilities and incidents continue to increase, and the remediation process often remains slow \cite{senDeterminantsSoftwareVulnerability2020, jafariDependencyPracticesVulnerability2023}. Moreover, Alfadel et al. found that, in the Python ecosystem, it takes a median of up to 4 months to fix all occurrences of a security vulnerability in the PyPI repository \cite{alfadelEmpiricalAnalysisSecurity2021}. Existing market solutions cannot be regarded as definitive protections against supply chain attacks as experts identified several additional drawbacks and limitations of current SCA tools \cite{dietrichSecurityBlindSpots2023, enckTopFiveChallenges2022, bohmeSoftwareSecurityAnalysis2025}:
\begin{itemize}
    \item focusing more on static analysis rather than on dynamic analysis during runtime execution;
    \item they operate within the local environment instead of isolated or remote setups;
    \item comprehensive deep scanning of transitive dependencies is often missing;
    \item insufficient integration with AI and data analytics-based systems.
\end{itemize}

% subpart: general limitations of software composition analysis and other form of data gathering, also context of malware analysis
% @TODO: add reference about "distribution formats and categories"
% @TODO: references to details of malware analysis (statis and behavioral analysis) principles from word document of fastsandbox (explaning what is malware/binary analysis and how it might be valuable for SCA)
% @TODO: figure "malware and сложность задачи их анализа для любого ПО в том числе"
% @TODO: аналогия из биологии что является вредным или безвредным
% @TODO: references to flaws of VirusTotal and anti-virus model - current anti-virus model is outdated and Google Play / Google Chrome extensions incidents. Static Scanner and File Analyzer, automated malware analysis toolkit, automated malware analysis capabilities. anti-analysis and bypass подходов: обфускация, пакетирование c полиморфизмом/метаморфизмом, анти-дебаг, uac bypass techniques, anti-sandboxing, anti-vm, anti-debug or вырезанная дебаг информация, добавлением мусорной логики, антидебаггинг, и менее известные или уникальные стелсовые техники.
% @TODO: extract more info from references provided is this subpart
Furthermore, obtaining reliable data about software remains challenging due to its various distribution formats and categories:
\begin{itemize}
    \item white box - fully known content (e.g., open source);
    \item gray box - partially known content (e.g., leaked information, rumors, reverse engineering, insider knowledge, alternative products, or forks);
    \item black box - unknown content (e.g., functionality behind API like SaaS, proprietary and closed-source software).
\end{itemize}
General SCA tools typically focuse only on the white box approach by gathering already indexed vulnerabilities or components, thereby leaving critical gaps in threat visibility and transparency. Evidently, software analysis and detection/identification for other categories (both malware and legitimate) are inherently more complex and complicated due to their specifics, because anti-analysis techniques are used to protect such software and intellectual property, making component-level inspection difficult or reasonably impossible \cite{zaidenbergTrulyProtectVirtualizationBased2022}. Although services like VirusTotal or similar antivirus platforms scan uploaded untrusted artifacts against numerous antivirus engines and databases to protect the corporate or personal sector, they cannot cover all possible forms of malware. For example, on platforms such as Google Play or similar repositories, there are many instances of hidden malware or fully undetectable malware (FUD) that have successfully bypassed the verification stage. Unfortunately, there is no universal solution or method against that, since software protection is constantly evolving. Therefore, it is mandatory to conduct additional reconnaissance and data gathering of existing samples and resources via well-known or stanrdard deconstruction methods. In the realm of malware analysis, objects that are stored or set for analysis are often termed as "IoCs" (Indicators of Compromise), "artifacts" or simply "samples" \cite{iklody_decaying_2018}. For example, these include entities such as IP addresses, URLs (links), DNS (domain names), virus signatures, hashes, files, and more. It is essential to clarify that the previously mentioned malware analysis is not limited solely to analyzing malware, and the term "malware" itself is quite vague/broad. Essentially, malware is any software which is considered potentially harmful or could lead to a security breach depending on specifics of our threat modeling with predefined set of malicious characteristics and behavior. Malware analysis itself is typically categorized into four main types, which can be combined in various ways: 
\begin{itemize}
    \item Static analysis focuses on examining the malware without executing it, which involves inspecting code, file structure, metadata, strings, and other non-executable aspects.
    \item Dynamic analysis, on the other hand, observes the malware in action by executing it in a controlled sandbox environment. This type of analysis monitors real-time / runtime behavior, such as network activity, file modifications, and registry changes, and is sometimes called "behavioral analysis" \cite{or-meirDynamicMalwareAnalysis2020}.
    \item Manual analysis involves hands-on inspection of code or source by an analyst, allowing for in-depth, targeted evaluation. 
    \item Automated analysis, on the other hand, leverages tools to systematically assess and classify malware or provide preliminary insights.
\end{itemize}

% subpart: changing the status quo and rethinking software ecosystems
% @TODO: add reference for "secure by default", "call down", "mandatory oversight" and expand bullet-point list with more ideas/content.
% @TODO: add reference to high fragmentation according to stats from https://repology.org/
% @TODO: объяснение для software licenses что они имеют малую юридическую силу и ничем не закреплены (де-юре может быть но де-факто это не так и много кто нарушает их в скрытую). Также пример с графом зависимостей у проектов (то что транзитивные зависимости могут иметь конфликтующие лицензии по сравнению с главной зависимостью и тогда в чем ценность лицензий вовсе). Также пример texlive и plantuml где зоопарк конфликтующих лицензий как фантиков от конфет. Additional explanation from me before: "software licenses в целом это формальность которая юридически не обоснованна. также если еще посмотреть по транзитивным зависимостям то там вылезит куча конфликтующих зависимостей"
The fragmented software ecosystem worsens this situation, making accountability for the many packages and their adherence to standards unclear \cite{enablingGlobalCollaboration}. For instance, Mirakhorli et al. reviewed 84 available SBOM management toolkits across multiple programming language ecosystems, which demonstrates the lack of unification and significant fragmentation in current solutions \cite{mirakhorliLandscapeStudyOpen2024}. Even software package that is trusted by the community cannot offer proper guarantees, because mainstream software licenses clearly mention that their products are distributed "as is" condition \cite{wintersgillLawDoesntWork2024}. Experts propose different solutions for the current status quo, ranging from weak to radical measures. Firstly, there should be an introduction of increased mandatory and strict basic requirements (baselines) at the platform level with compliance from end user side. This could reduce the number of attacks and malicious code due to high standards and improved quality. It also reflects the principle of "secure by default", where secure policies and good default configurations should become the common for many, and not just the exception reserved for professionals:
\begin{itemize}
    \item Mandatory oversight and analysis before package publication and subsequently with every update/change. For example, establishment of a moderation system where the community will naturally report and react to appearance of malicious packages. Such movements as bug bounty programs can provide payment to the community for finding bugs to establish natural vulnerability research and embrace ethical hacking over illegal activities. \cite{bozziniRegimesEthicalHacking2025}. Also, there is a productive case within Astana IT University where they regularly implement CTF (Capture The Flag) and Cyberpolygons to train security engineers, thereby validating skills and preparing professional specialists for organizations \cite{abdiramanComparativeAnalysisApplication2023}.
    \item Policies for temporary blocking or issuing "call down" periods between user actions during anomalous activity (e.g., IP address change, password or specific token renewal, recent publications, and so on). % @TODO: references
    \item Movement towards slowing down of overproduction and starting to "recycle waste" by rethinking old or abandoned ideas (re-evaluating them, finding patterns, and recognizing repetition). Perhaps, developers should also strive to regularly improve the standard library and make it better with "batteries included" in the official distribution. Otherwise, its position will be taken by many competing dependencies covering the exact same specific topic, thereby generating a high degree of entropy. % @TODO: references
    \item Designing systems in a way that they are "secure by design" in their foundation means that security and safety are built-in initially rather than being an additional element. This approach dictates that engineers strive to place security and features at least on the same level of value during the development process. It eliminates specific categories of vulnerabilities, restricts/limits malware, and forms a multilayered security / defense-in-depth approach \cite{mellMeasuringImprovingEffectiveness2016}. % @TODO: add reference "secure by demand" before mellMeasuringImprovingEffectiveness2016, and https://arxiv.org/pdf/1507.02992
    \item Next-generation and future-proof system that are comprehensive to the extent that they can self-analyze and self-improve, making them more reliable and less fragile. For instance, Berhe et al. showed that companies are compelled to have a mitigation plan and a regular budget for automated software dependency recovery or manual actions such as replacing it with alternatives, upgrading it, or reverting to a previous release \cite{berheMaintenanceCostSoftware2023}. Ideally, this issues could be mitigated by well-decomposed systems that might still continue to function in such critical cases or have internal recovery mechanisms ("self-healing") for the failure of individual components only up to a certain limit \cite{dehrajReviewArchitectureModels2021}.
\end{itemize}

% subpart: regulations, hidden malware (Ken Thompson's hack), distinction between design and implementation, old model of trust (trusted/semi-trusted/untrusted)
% @TODO: add reference to https://arxiv.org/pdf/1803.07244 and to programming language model of execution (C++ abstract machine)
Secondly, the software market should pay more attention and show greater concern for formal regulatory mechanisms to increase trust and responsibility in the community, such as certification, auditing, and transparent processes, along with initiatives aimed at improving standards. Sammak et al. attended an interview among a small subset of experienced developers, which revealed that the majority of respondents are unfamiliar with details of software supply chain management or have a superficial understanding due to the absence of proper training on this topic for a broader audience \cite{sammakDevelopersApproachesSoftware2023}. Thirdly, popular or operation-critical software have a strong influence on whole landscape, so compromising them can undermine not just dependent infrastructure but also the global contracts of security, such as cryptography stack \cite{tsoupidiProtectingCryptographicLibraries2025}. For example, self-replicating malware or virus might compromise a significant portion of the IT ecosystem by ensuring that any code compiled on an infected hardware platform, compiler, or OS becomes compromised as well \cite{ladisaTaxonomyAttacksOpenSource2022}. This is due to the fact that definition files are merely instructions without guarantees that our intentions will be correctly expressed and transformed with preserved semantics in the produced output (e.g., machine code, process, and other representation) by intermediary system (compiler, interpreter, execution environment), as shown in Figure \ref{fig:intermediary_system_between_design_and_implementation}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{intermediary_system_between_design_and_implementation}
    \caption{Intermediary system between design and implementation.}
    \label{fig:intermediary_system_between_design_and_implementation}
\end{figure}
Furthermore, conducting a compromise assessment or software evaluation for the produced product is complicated, because infected reverse engineering tools (e.g., debugger, decompiler, disassembler) or even their clean versions running in a compromised environment might hide the infected parts and backdoors as blind spots in the software in order to remain undetected. Due to the high stakes, it is crucial to determine whom to trust and based on what principles to minimize such risks beforehand. Researchers Singer and Bishop propose viewing trust in the form of a relationship model that makes us vulnerable because we rely and depend on a third party without any doubt due to assumptions and distractions, considering them trustworthy \cite{singerTrustBasedSecurityTrust2020}. It means that our inherit tolerance to possible flaws/mistakes due to the factor of trust affects on our cybersecurity posture. They also suggest applying threat modeling and risk analysis before and during the design stage in the development lifecycle to minimize the factor of trust. Hence, trust should not be considered as a static and accurate indicator, but rather it is a dynamic probability of being trusted by someone else and vice 
versa, where any mistake from both parties leads to its loss. For example, one of the parties (recipient or sender) can at any time disrupt data transmission and its correctness by intentionally beginning not to adhere to the given standard protocols and interaction interfaces.

% subpart: zero trust as theory and in practice (e.g., blockchain)
For these reasons, the National Institute of Standards and Technology (NIST) developed the concept of Zero Trust Architecture (ZTA), in which trust requires continuous verification \cite{roseZeroTrustArchitecture2020}. Babenko et al. clearly demonstrated that empirical data on ZTA implementation correlates with a lower incident rate of insider threats \cite{babenkoCybersecuritylevelAssessmentModels}. Organizations can adopt this by following the key principles listed below:
\begin{itemize}
    \item Continuous evaluation of all actions and actors instead of granting full access within a protected perimeter. This means the environment itself does not affect how we trust someone/something, assuming operating in a hostile environment. % @TODO: reference to exact part about this from ZTA document 
    \item Eliminating implicit trust in favor of explicit trust using a trust algorithm as a method for verifying access or credibility with transactional properties \cite[Ch. 3.3]{roseZeroTrustArchitecture2020}.
    \item Segmentation of the network and systems into atomic cells, which theoretically fragments the attack surface (e.g., a large monolith will expose more data than individual microservices), as visualized on Figure \ref{ref:network_segmentation_in_zero_trust} \cite{JourneyZeroTrust}.
    \item Least privileges of access and minimizing amount of dependencies via strict comprehensive assessments of vendors or suppliers \cite{collierZeroTrustSupply2021}. On top of this, it is recommended to apply DevSecOps practices for the automation of software building, testing, and the overall "shift-left" movement for the early identification of problems in IS (Information Security) \cite{leeDoDEnterpriseDevSecOps}.
    \item Security as a dynamic process and adaptive system achieved through the continuous collection and analysis of data/statistics (resources, assets, incidents, etc.). % @TODO: также референс на explainable AI и контекст анализа данных в ZTA \cite{mushtaqSystematicLiteratureReview2025}, ever changing attack vectors (add reference pourmoafiSolutionSecuringInformation and mengCybersecuritySimondonsConcretization2022)
\end{itemize}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{network_segmentation_in_zero_trust}
    \caption{Network segmentation in ZTA, where each segment consists of multiple defense layers and granular microservices.}
    \label{fig:network_segmentation_in_zero_trust}
\end{figure}
One example of ZTA implementation is blockchain systems, which contain zero-knowledge proofs and consensus algorithms such as "proof of work" and others that guarantee trust based on a defined trust factor (e.g., computational power, number of devices/actors, amount of specific tokens) \cite{banguiWhenTrustlessMeets2024}. Furthermore, there is a concept of "trustless" in the same sphere that futher reinforces ZTA and trust management approaches in distributed systems. Some studies even consider the potential for applying blockchain as a version control system due to its data integrity and non-repudiation characteristics \cite{grilliCombiningGitBlockchain2024}.

% subpart: conclusion of literature review to understand research gap in current landscape
% @TODO: расширить эту часть подробнее как подведение всех итогов and identified research gaps in current approaches / topic
Overall, understanding the current problems and limitations provides us with further perspectives for developing the processes of creation, verification, and analysis of the contents of IS and IT infrastructures. Moreover, large or critical incidents will encourage community to consider how to prevent them and fundamentally revise current processes that are recognized as entirely insufficient.

\section{Methodology}
% @TODO: expand this part and decompose to more subsections (depending on problem and components not just one large section). Методы (методология исследования) – методы, используемые для проверки гипотезы, должны быть описаны достаточно подробно, чтобы другой исследователь в этой области мог повторить проверку. Methodology is a design that you are using to answer research questions and research gaps. “I am going to use these techniques and this type of analysis to solve some problem”. Methodology (design, framework, approach) - big umbrella consists of multiple methods underneath (количественный анализ - сбор данных; качественный анализ - what, how, why, where)

\subsection{Proposal concept and paradigm shift}
% subpart: intro to the paradigm shift with sandboxes for SCA
The methodological part focuses on addressing problems and gaps identified in the literature review of the current landscape, which indicates that a change in the paradigm of perception and understanding (studying, analyzing, and defining) of problematic components in IT infrastructure is necessary, including both malware and legitimate components. Therefore, this study propose to adopt principles from existing malware and binary analysis tools and digital forensics, such as sandboxes and others, which can be integrated into the process of collecting and analyzing components from IT infrastructure. These toolkits can effectively complement modern SCA approaches and enable a paradigm shift, as illustrated in Figure \ref{fig:paradigm_shift_for_sca}. The proposed idea involves redirecting the purpose of existing solutions toward a different target audience and analytical focus from regular end users to developers. Although this may appear to be a minor shift, it is both critical to have and straightforward to integrate.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{paradigm_shift_for_sca}
    \caption{Paradigm shift by applying malware and binary analysis for SCA-related tasks.}
    \label{fig:paradigm_shift_for_sca}
\end{figure}
Software systems based on these types of malware analysis are highly popular among information security companies, as they enhance efficiency and enable the scalable analysis of potential threats - a critical factor given the rising volume and sophistication of malware today. Companies leverage these tools to strengthen their cybersecurity capabilities, safeguard sensitive information, and enhance their response to potential security threats. Organizations often deploy these systems in various configurations, including on-premises (local) installations and SaaS models. The on-premises model involves deploying the system locally within the organization, typically to meet data confidentiality and security requirements. This setup can be integrated seamlessly into the organization's infrastructure, automated systems, and third-party services via APIs, enabling a customized and secure workflow. Often, open-source solutions like Cuckoo Sandbox and similar alternatives are employed in on-premises deployments. These systems are essential tools for a range of specialists, including reverse engineers, malware researchers, RnD (research and development) analysts, and SOC analysts, who use them to gain in-depth insights into malware behavior and potential vulnerabilities. On the other hand, SaaS solutions, such as VirusTotal and similar services, are provided as external, third-party resources. These platforms allow users to upload and analyze files or links for potential threats without needing to host the analysis software locally. SaaS solutions are commonly used by individual users or organizations with less restrictive security policies, offering a convenient, web-accessible approach to malware analysis. These services are especially beneficial for users who require quick, on-demand assessments without the need to invest in or maintain specialized infrastructure. However, due to the nature of third-party hosting, there might be data confidentiality considerations, making them less ideal for organizations with stringent data security requirements. Despite this, they provide valuable insights and contribute to the broader cybersecurity landscape by aggregating and analyzing threat data from multiple sources worldwide.

% subpart: expanding this idea of sandboxes to whole digital twins as platform for simulating scenarios
% @TODO: add more strong proof and general references to this part (про особенности со стороны качества и примеры применения из уже существующиех компаний использовали это и каким образом) in context of digital twins and sandboxes
Sandboxing approach is validated by NIST for ZTA, where it is recommended to manage software examination within isolated execution environments before allowing to add or change any component into the IT infrastructure \cite[Ch. 3.2.4]{roseZeroTrustArchitecture2020}. However, only passive analysis with sandboxes and honeypots is limited, because security engineers cannot predict some situations due to small subset of analyzed artifacts and insufficient data. Also, the behavior observed in a sandboxed environment is not always representative of the real system. Therefore, organization should strive to simulate the original infrastructure and everything in it to be exactly precise to the production instance in order to obtain more correct and realistic results in general, as demonstrated in Figure \ref{fig:transition_to_continuous_security_cycle}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{transition_to_continuous_security_cycle}
    \caption{Transition to cycle of continuous security (SOC - security operations center; MSSP - managed security service provider): (1) Passive analysis with sandboxes and honeypots. (2) Reactive analysis with digital twins of IT infrastructures.}
    \label{fig:transition_to_continuous_security_cycle}
\end{figure}

% @TODO: тут добавить про industrial and physical digital twins и потом сделать уже переход на software/cybersecurity digital twins чтобы была понятна аналогия. "Digital twins можно использовать также для проверки физической защиты предприятия (где можно проникнуть, подключить провод, пролезть, поломать проход, и т.д. на базе 3D модели реального здания или офиса)".
Full-scale digital twins allow to create copies of specific workstations up to entire large systems as complex real-time simulations. They might significantly reduce the time and cost of testing and validation, making the process truly continuous and assisting to identify critical problems before publication into production. In essence, such an analysis laboratory establishes a validation and feedback loop against constantly expanding attack surfaces, where security specialists can simulate multiple attack scenarios and gather all data from the IT infrastructure. It also provides following features and opportunities:
\begin{itemize}
    \item Digital hygiene via mirrored environment that is restricted and remote; % удаленные или Изоляция и безопасность. При parsing, downloading, and executing of untrusted files/software from external resources, они могут содержать malicious artifacts or что-то в целом может повредить или атаковать нас в локальное решение. Connection secured by VPN tunnel encryption, so we can безопасно использовать удаленный вызов процедур (RPC), подключение по SSH, передача по HTTP/HTTPS, и т.д. Ограничить такой инстанс от всех возможны соединений (firewall) во вне или запретить ему самому подключаться во вне (если произойдет reverse shell к примеру). делать так чтобы bad actor не мог повлиять или скомпроментировать сбор данных (к примеру, в пакете лежит скрипт который мешает установленному агенту собирать данные или подменяет данные которые мы собрали).
    \item Regularly deconstructing and reconstructing IT infrastructure from self-documenting files; % and allowing to customize it for dynamic end-to-end testing, what-if analysis, and chaos engineering.
    \item Immediately destroying an instance and clearing its resources upon completion of a task execution, meaning it is being constantly recreated by a new one as temporary and immutable (similar to short-living or stateless serverless functions). Such mechanism minimizes the risk of sandbox escape, preventing a malicious actor from pivoting into internal infrastructure through the sandbox environment. % @REFERENCE: vm/container/sandbox escape
    \item Portability and compatibility across various targets (device architectures, hardware combinations, OS platforms) or even capabilities for emulation.
    \item Extensible distributed systems which are made of multiple server instances with virtual machines and containers based on different forms of hosting.
\end{itemize}

% subpart: ai agents and general ai / data-driven solutions in context of digital twins.
% @TODO: citation to https://assets.anthropic.com/m/ec212e6566a0d47/original/Disrupting-the-first-reported-AI-orchestrated-cyber-espionage-campaign.pdf where using generative ai because if we have cheap and easy multiple PoC scripts it is enough to break something instead of creating качественные SOTA solution
% @TODO: add more explanation and examples of current ai agents or agentic ai state of the art because it is unclear. "можно расширить на другое виды симуляций (реального рынка, событий, игр, распределенных систем, мест, других приложений и т.д.). Using game engines not for games but for simulations.". Можно будет делать копии всей инфраструктуры заране и помещать туда множество (сотни и тысячи) AI agents которые будут тестировать все в автоматическом режиме активно проверяя систему.
Furthermore, with the raising relevance and progress of AI systems, automated and autonomous solutions based on multimodal AI agents (a combination of language, vision, etc.) are increasingly being integrated. These agents could be utilized as lightweight pentesters under customizable scripted scenarios. It is more than brute-forcing or fuzzing because they are flexible and their behavior is stochastic (similar to real users). Such intelligent agents can conduct user/developer-like interactions or simulate entire communities as natural traffic inside a digital twin over an extended period of time, thereby revealing hidden aspects of software during runtime, and finding weak points or unpredictable states of IS within extreme conditions. Also, they can be fine-tuned and configured for specific tasks by defining: behavioral patterns, rules, instruction or goals (e.g., according to use cases or user stories), APIs and tools, their simultaneous amount, etc. Overall, it is a healthy botnet operating for benefit and risk prevention rather than harm to production systems.

\subsection{System modeling and operating mechanisms}
% subpart: introduction to theoretical concept
% @TODO: too abstract and vague - need to clarify
There is provided only a theoretical concept of specific foundational elements of the solution and their description without strict details to technical aspects in order to be technology agnostic and portable to other ecosystems. The problem domain was decomposed into several subtopics that naturally align with required components and services, ensuring that it remains both extensible and customizable. The architectural design was based on the principles of hexagonal architecture, because it provides a clear separation between supplementary elements and the core business logic, such as driving side, application, and driven side. This model introduces the concept of adapters and ports, borrowed from electronic engineering, which enables the replacement or modification of component implementations. 

% subpart: driving side of the architecture
% @TODO: expand this part with description of openapi specification for such solution and library scripting api - for native interaction and scripting with application/platform. other forms of interactions
The driving side represents the system's entry points, including regular users and external automated systems that interact with the platform through backend APIs and web interfaces over network connections within client-server model, as shown in Figure \ref{fig:driving_side}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{driving_side}
    \caption{Driving side of the architecture.}
    \label{fig:driving_side}
\end{figure}

% subpart: internal part of application (services and subsystems)
The internal part of the application is designed as a modular monolith, divided into multiple components or services, as illustrated in Figure \ref{fig:internal_application_components}:
\begin{itemize}
    \item "Identity provider, user and session management" - maintains all information about users in the platform with their metadata. Generally, "Authentication" and "Authorization" operate alongside with that, forming a comprehensive set of components responsible for user-related security operations: roles or access control based on permissions, login and register handling, access/refresh token management, checking behavior and actions of user, multi-factor authentication (MFA), and so on.
    \item "Groups and project system" - holds multiple analyses in which users can collaborate with each other.
    \item "Moderation system" and "Administration system" - are intended for platform operators, enabling internal system management and response to user requests as needed.
    \item "Analyses system" - is a critical component that performs all essential operations required for software analysis.
\end{itemize}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{internal_components_of_application}
    \caption{Internal components (i.e., services or subsystems) of the application.}
    \label{fig:internal_components_of_application}
\end{figure}
Components can be added, modified, or disabled as needed. This flexibility improves security by reducing the attack surface, minimizing system states, decreasing computational complexity, and factor of combinatorial explosion. The analysis engine also features a plugin and modding system with multiple operating modes. This means that users of the system can develop their own plugins for specific use cases by implementing the provided interface. Such extensibility enables the analysis not only of software packages but also of container images, computer networks, and other parts of IT infrastructure. Furthermore, there is support for fine-grained configuration and operation modes. For example, specifying which data to collect or ignore, potentially through regex-based filtering.

% subpart: how analysis engine works
% @TODO: microkernel and plugin/modular architecture (and about IPC communications). how orchestrator общается с каждым из элементов из bundle, или оркестратор общается лишь с bundle абстракцией который уже resolving остальное. How someone will became Publisher from Guest and how some one will se published packages (maybe someone can show them in public, and other will be by default in private) - list of analyses. But what if analysis will be too much (типа можно слишком много делать анализов на одни и те же пакеты) - попробовать посмотреть как делается у других систем
% @TODO: modification of system (redraw as UML Class diagram short with bundle example for plugin system)
The analysis engine consists of tightly coupled contextual components, all operating around a common abstraction of "task", as seen in Figure \ref{fig:analysis_engine}:
\begin{itemize}
    \item Orchestrator - is responsible for task queuing, scheduling, and overall management operations. It serves as an intermediate layer that receives tasks directly from the analysis system, each with specific requirements that must be resolved before delegation to other components. For example, there are possible types of task requirements such as package (single dependency or group of dependencies within one or multple project in VCS and archive/compressed format), microservices (container image or layers), and many other supported data, resources, and infrastructure. At the same time, Orchestrator manages and coordinates the overall execution process, acting as the central communication and control hub, because components in the Bundle cannot begin execution without confirmation or explicit request from Orchestrator. The components inside the Bundle are a form of "utility modules" that perform a specific task, and they are not limited to the presented standard set, meaning someone can create a custom module on top of it. Also, inside the bundle there are often common and internal libraries and other code scripts that can be reused for utilities (e.g., scripts for WinAPI). In this context, Orchestrator interacts with the Bundle abstraction rather than directly with its individual components, which contains a set of predefined task executors as described below. Also, the connection between Orchestrator and Bundle follows a one-to-many relationship, where Orchestrator determines which Bundle to use among several available ones, as provided in Figure \ref{fig:customizable_bundles}.
    \item Instancer (i.e., Provisioner) - creates, prepares, and manages the required environments (e.g., containers, VMs, specific OS or images, etc.).
    \item Collector (i.e., Extractor) - continuously or periodically collects data from created instance (e.g., logs, files, making image screenshots or recording videos, memory or disk snapshots, etc.).
    \item Interactor - performs predefined set of actions or scripted scenarios in the isolated environment to simulate user behavior and trigger malware responses (e.g., interaction with file system, launching or installing programs, managing input/output devices, etc.).
    \item Parser - transforms the required data into a unified format, such as an intermediate representation (IR), or converts it into a format suitable for Analyzer.
    \item Analyzer - produces specific verdicts or predictions based on the received data (e.g., determining critical components within dependency tree, performing other specialized analyses using both stochastic and deterministic algorithms).
\end{itemize}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{analysis_engine}
    \caption{The engine of analysis system.}
    \label{fig:analysis_engine}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{customizable_bundles}
    \caption{Customizable bundles for specific cases.}
    \label{fig:customizable_bundles}
\end{figure}

% subpart: driven side of the architecture
% @TODO: Подробнее описать за что каждый компонент из driven side отвечает и зачем он там действительно
% @TODO: agents as specific software on endpoint to transfer data or control endpoint
Figure \ref{fig:driven_side} illustrates the components of the driven side along with some generic infrastructure elements that are required for proper application functionality. Each of them fulfills a specific role, but this can also be extended with other external integrations if necessary. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{driven_side}
    \caption{Driven side of the architecture.}
    \label{fig:driven_side}
\end{figure}
Especially, principles of connectivity to artificial instances is represented with agent-based or agentless strategies ("push" and "pull") on Figure \ref{fig:agent_and_agent-based_strategies}. Both approaches have distinct advantages and disadvantages regarding data transmission, depending on the entity responsible for triggering or handling the transfer. Specifically, agent-based methods necessitate the installation of supplemental software on every endpoint, whereas agentless methods allow connection via a single collector (or centralized point). Ideally, organizations should be offered both approaches to allow them to determine which is more suitable for their operational and maintenance needs.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{agent_and_agent-based_strategies}
    \caption{Agentless and agent-based strategies.}
    \label{fig:agent_and_agent-based_strategies}
\end{figure}

\subsection{Practical application of the methodology}
% @TODO: расписать что такое примение может быть стандартизировано, то есть необходим единый стандарт of continuous security и swagger API for such system, instead of multiple разные ПО like Cuckoo и его альтернативами которые все сделаны по своему. Добавить, если возможно, псевдокод или сценарий применения: например, показать шаги анализа конкретного пакета или библиотеки через предложенную систему;
% @TODO: Указать более детально какие метрики (для сбора данных и инструменты), какие пакеты и сколько (описание датасета) -> "all gathered additional data (from runtime, files, hidden dependencies, and so on). But there are certain categories of software that we cannot test or limited (API clients to saas products because it would require some account- but such API clients would be checked statically and will be marked as potentially untrusted)"
% @TODO: В результатах уже показать в табличном виде что нашлось и где больше и качественнее. Gather also malware datasets and other datasets with packages. 

% @TODO: add image secure_software_pipeline.pdf (applying_zta_and_security_for_software_pipeline to дополнить SCA in UML sequence diagram). add image overview_of_software_analysis_process.pdf (process of software analysis details within sandboxed environment in UML sequence diagram). Also proper explanation for them and what to change in their prototypes as comments
С практической точки зрения ранее предложенные подходы уже сейчас даже можно интегировать в existing systems for software repositories to enhance SCA and 
сформировать principle of continuous security, ведь sandboxes хорошо применимы для лабораторного исследования ПО или like its containmenation before publications or changes. Для этого был подготовлен case как можно легко модифицировать репозитории пакетов (i.e., PyPI), чтобы на протяжении всего жизненного цикла ПО или других компонентов IT infrastructure они посылались на анализ в развернутый sandboxes. The analysis emphasized the external perimeter while also exploring specific elements of the PyPI platform, which already demonstrates several comparable and thematically relevant aspects. It hosts a large number of different Python packages and is owned by the Python Software Foundation (PSF) which operates under a free and open model, where any user may use the package manager or other mechanisms to upload, download, and publish packages in the form of source or build distributions. Similar systems and policies are followed by other software repositories and ecosystems, such as Maven Central, NPM, Crates, and many others. Moreover, the prototype concept demonstrates a variation that can be implemented independently of specific technologies, making it a technology-agnostic and portable solution (ведь принципы и методологии первичны а инструменты вторичны). Поэтому, попробовали протестировать данную методолгию уже на существующих инструментах для gathering data and attending experiment by applying existing solutions (like Cuckoo Sandbox и схожих решениях).
% --------------------------------------------------------------------------

\section{Results and discussions}
% @TODO: Результаты – гипотеза должна быть проверена, а данные, представляющие результаты проверки, представлены. (обязательно графики и таблицы с результатами данных). результат это алгоритм, метод, модель, архитектура для статьи и также обоснование. Критика системы и недостатков и сравнение с другими альтернативами и то что мы не покрыли в research. comparative analysis
% @TODO: introduction to results - "The results include an analysis of security and critical decisions derived from the proposed methodology as well as a comparison with existing market solutions addressing the same topic".

% subpart: results of data gathering with statistics in form of tables
% ...

\subsection{Threat modeling and risks}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{}
    \caption{DFD legend}
    \label{fig:placeholder}
\end{figure}
Also, there was prepared the DFD (data-flow diagram) legend with all necessary elements for the drawing process, as represented in Figure 9. Moreover, constructed level 0 DFD as shown in Figure 10. It was consciously decided to omit aspects concerning connectivity to the internet (e.g., internet service providers and intermediary network components), as those elements fall outside the scope of this analysis.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{} % span across 2 columns wide at the start of page
    \caption{Level-1 DFD (interaction interfaces with the platform)}
    \label{fig:placeholder}
\end{figure}

\subsection{Tradeoffs in the architecture prototype and general limitations}
% @TODO: Тут также добавить про то что системы будет открытой и любой сможет поучаствовать в развитии проекта в дальнейшем и его философия (что его отличает от конкурентов). Need references and examples of such toolkits. Add table сравнение с другими решения и анализ рисков (потенциальные риски)
% @TODO: Сравнить и привести примеры как работают элементы других систем (SOTA solutions) для сравнения со своей раскрывая аспекты и характеристики, постепенно вычленяя определенные элементы которые слабо-развитые в их контексте
This section outlines the critical architectural decisions made and the reasoning behind them. Firstly, it was decided to keep the Parser and Analyzer separate to ensure better decomposition; however, merging them into a single entity could improve performance and efficiency. Secondly, to reduce the load and prevent bottlenecks on a single Orchestrator, multiple instances of the Engine should be deployed, or the system should run in a concurrent or parallel mode. Thirdly, it was decided to group task executors into a single Bundle, as these components were found to be highly interdependent. Initially, they were considered as separate entities without direct communication, following a microkernel architecture in which component interaction occurs asynchronously (via message bus and inter-process communication). However, this approach proved unsuitable, since Collector depends on the specific implementation of the data source, Parser requires awareness of data format, and Analyzer cannot detect anomalies or draw conclusions from incompatible input data.

% subpart: potential flaws in digital twins and sandbox approach
% @TODO: Add reference to ahmedReviewHybridDeep2024 (about side-channel attack) in flaws of sandboxed environments
% @TODO: уязвимости of ai agents like cross-prompt injection (https://learn.microsoft.com/en-us/windows/security/book/operating-system-agentic-security)
% @TODO: reference на то как Capev2 ротирует характеристики VM в реестре и github.com/zhaodice/qemu-anti-detection
% @TOO: virtualization inside virtualization is slower, limits of Nested virtualization, and so on
Sandboxes тоже не ultimate solution и у них есть свои limitations. Firsly, симуляция это лишь поверхностная модель и не всегда представляет полную картину мира - поэтому отклоняется от реальности и недочеты. Secondly, возможны уязвимости с побегами из untrusted изолированного окружения в другие зоны IT инфраструктры. Thirdly, эмуляция и виртуализация отличаются от реального железа по характеристикам и качественным свойствам. Конечно, можно подставлять характеристики реального устройства и ротировать для маскировки, но реальное физическое устройство ведет себя по другому в некоторых случаях. К примеру, есть side-channel атаки и некоторые уязвимости физического железа которые не будут проявляться или наоборот. Поэтому, некоторое ПО пользуется этими особеностями для определения нахождения себя в таком искусственном окружении тем самым скрывая свое поведение в нем или запрещая свою работу (e.g., SEB). Это потребует дополнительных манипуляций таких как пересобирать исходник, патчить или дебажить бинарник, и т.д. 

В целом, в таких задачах мы пытаемся найти корневую логику или восставноить ее или ключевые моменты и специально запутанного artifact. Текущая парадигма в индустрии не является конечной, всегда найдутся другие и более инновационные которые переосмыслят все устои или дополнят их. Возможно такой domain задач можно отнести к encryption from cryptography где происходит решение сложных задача для запутывания но сам исходник находится внутри но в усложненном представлении, где можно ускорить descryption за счет GPU или quantum computing (для разворачивания запутанности malware на самые базовые компоненты или около исходный вариант который будет в разы проще анализировать). Также стоит задавать вопросы возможности решения анализа malware или ПО в математическом представлеии за счет доказания наличия определенной корневой логики которая является malware-подобной но без проведения анализа исходных файлов мы не можем проанализировать.

However, maintaining and advancing these systems or tools poses a range of challenges related to outdated technologies and architectural limitations. Many of these issues are rooted in the inherent complexity of the domain, as effective malware analysis requires not only a solid understanding of web technologies but also a deep knowledge of bare-metal and low-level system aspects. This complexity, combined with inadequate team preparation, often leads to the following issues. Firstly, messy and mixed components with poorly separated layers without clear layer separation results in a disorganized codebase, making it difficult to manage, troubleshoot, and extend the system effectively. Secondly, poor documentation and lack of compliance can lead to absence of adherence to industry standards or best practices that could increase security risks and reduce system reliability. For example, incident involving undocumented features in CrowdStrike's Falcon software impacted multiple Windows-based systems (https://www.crowdstrike.com/wp-content/uploads/2024/08/Channel-File-291-Incident-Root-Cause-Analysis-08.06.2024.pdf). Thirdly, limited capability for modifications and custom integrations in these systems hinders flexibility, making it challenging to add custom modules, integrate with external tools, or adapt to emerging malware threats and evolving analysis techniques. Fourthly, possible accumulation of redundant or outdated libraries and dependencies increases security vulnerabilities and maintenance costs, making regular audits essential to ensure a streamlined and secure environment. Fifthly, sophisticated malware tactics present ongoing risks of virtual machine / sandbox escape and evasion, requiring continuous updates to ensure the analysis environment remains secure. To handle the latest malware threats effectively, the core modules of the analysis system need enhancements in detection techniques, behavior monitoring, and analytical capabilities. Regular improvements in these areas are crucial for maintaining the system’s relevance and effectiveness in detecting and responding to advanced threats.
% ----------------------------------------------------------------------------

\section{Conclusion}
% @TODO: данные должны быть обсуждены, результаты интерпретированы и сделаны выводы.
% @TODO: expand this part with more details about future work which is required
The topic of supply chain management in IT infrastructure remains highly relevant, as the number of related incidents continues to grow despite the emergence of new tools and standards. More fundamental, preventive measures may be required at the source level, where software packages are produced, rather than reacting after incidents occur.

Future research will focus on converting the proposed methodology into a microservice-based system, which remains a challenging task due to its complexity and management requirements. However, such an approach would enable scalability and continuous integration of new functionality without interrupting the platform. Further work will also include a full implementation of the system and an independent statistical analysis of software dependencies beyond those examined in the current literature review.

% \nocite{*}
\printbibliography

\end{document}
