% General styles and packages: (TODO: move preamble to separate file)
\documentclass[conference, comsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[none]{hyphenat} % turn off hyphenation
\setlength{\parindent}{0} % no space between paragraphs
\graphicspath{{figures/}} % default path for figures

% References: (TODO: migrate to bibtex because IEEETran has not fully correct styling for biblatex)
\usepackage[backend=biber, style=ieee, sorting=none, url=false]{biblatex}
\addbibresource{references.bib}
%% IEEE bibliography styles for biblatex:
\renewcommand*{\bibfont}{\footnotesize}
\setlength{\biblabelsep}{\labelsep}
\setlength{\bibitemsep}{\IEEEbibitemsep}

% Heading: (TODO: \usepackage{fancyhdr})
\makeatletter
\def\ps@IEEEtitlepagestyle{
  \def\@oddhead{\parbox{\textwidth}{
    {\footnotesize
    2026 IEEE 6th International Conference on Smart Information Systems and Technologies (SIST)\\
    13--15 May 2025, Astana, Kazakhstan\par}
  }}
  \def\@oddfoot{}
}
\makeatother

\begin{document}
\title{Customizable methodology for in-depth analysis of software within isolated computational environments to secure critical IT infrastructures\\}

\author{\IEEEauthorblockN{1\textsuperscript{st} Noyan Tendikov}
\IEEEauthorblockA{\textit{School of Cybersecurity} \\
\textit{Astana IT University}\\
Astana, Kazakhstan \\
242710@astanait.edu.kz \\
ORCID: 0009-0009-7251-8830}
} % TODO: add rzaeva and lishnevsky to authors?

\maketitle

\begin{abstract}
% TODO: Refine abstract - краткое и объективное резюме, которое представляет собой предварительный обзор остальной части статьи. Оно должно быть лаконичным, но при этом предоставлять достаточно информации о статье, чтобы облегчить принятие решения о том, будет ли статья полезна для прочтения. В абстракте чисто будет очень краткое объяснение что сделали и какие результаты, в какой теме, зачем
Critical IT infrastructures are increasingly threatened by hidden attacks targeting their components. Malicious actors can infiltrate silently through software dependencies by launching an indirect attack on internal vulnerabilities. This research aims to explore this issue by focusing on software composition and process unification, extending the current approaches through a prototype framework for analyzing software in specialized isolated environments. The preliminary results of risk assessment and threat modeling demonstrate its viability and security properties. Further refinement and studies are required, including a reference implementation, a comprehensive technical specification, and iterative field research with detailed statistics based on the implemented approach.
\end{abstract}

\begin{IEEEkeywords}
software supply chain; infrastructure-as-code; critical IT infrastructure; software composition analysis; software dependencies; software bill of materials; sandboxing; malware and binary analysis; digital twins; intelligent agents
\end{IEEEkeywords}

\section{Introduction}
% TODO: Refine introduction to be whole page without and more solid (цельным повестованием as flow)

% current sutation about escalation of conflicts that affect on cybersecurity of critical infrastructures
Modern cyber conflicts have escalated into a condition of constant cyberwar, driven by countless threats. Unlike traditional military conflicts, this warfare is taking place within a digital space and often goes unnoticed by the general public due to their lack of awareness and competence \cite{almeidaComparativeAnalysisEUbased2025}. The current policy of major states is dictated by the principle of "peace through strength", prioritizing the rule of power over ethical considerations and international law. It leads to an intensified arms race between multiple actors, such as government agencies, armies, corporations, and private groups. Under these circumstances of ongoing confrontations between defenders and attackers, ensuring security is only a continuous process rather than an absolute guarantee \cite{safitraCounterattackingCyberThreats2023}. So, the industry of information technology (IT) should be engaged in systematic risk assessment and apply reactive patches as problems emerge, because the number of potential threats and vulnerabilities is inherently unknown and cannot be fully identified or eliminated in upfront. Moreover, the increasing rate of automation and digitalization exposes associated risks for critical infrastructure, where their compromise could create dangerous situations and collateral damage for citizens \cite[pp. 1--2]{nationalinstituteofstandardsandtechnologyFrameworkImprovingCritical2018}. For example, the Cybersecurity and Infrastructure Security Agency (CISA) of the USA adopted a classification that identifies 16 critical infrastructure sectors \cite[pp. 6--7]{InfrastructureResiliencePlanning}, including the context of IT infrastructures. This sector is composed from corresponding services and assets (e.g., maintenance personnel, electricity, networks, equipment, hardware, software), providing an operational basis or computational environment to utilize for information systems (IS) \cite{qatawnehMediatingRoleTechnological2024}.

% reasons and factors of attacks on critical infrastructure and their consequences
Fundamentally, attacks targeting critical IT infrastructure are driven by diverse motives and reasons, intersecting with other industries \cite{lehtoCyberattacksCriticalInfrastructure2022}. Historical events have already shown that conflicts of national interests might trigger a security breach on highly protected facilities (e.g., nuclear power plants) \cite{makrakisIndustrialCriticalInfrastructure2021}. For instance, Riggs et al. examined statistics on global cyberattacks against critical infrastructures since 2006 and developed a prediction model that shows exponential growth of incidents with the military sector being the most frequently targeted \cite{riggsImpactVulnerabilitiesMitigation2023}. Another factor that may escalate the situation is the growing trend around generative artificial intelligence (AI), which increases the demand on data centers, stimulating competitors and other third parties to compromise or abuse that computational power for their own benefit \cite{yigitCriticalInfrastructureProtection2024}. Additionally, the growing interconnectivity of IS, especially through Internet of Things (IoT) devices, causes fragility by exposing multiple points of failure and allows malicious actors or even device manufacturers to gain remote access to them \cite{vardakisReviewSmarthomeSecurity2024, rauhalaPhysicalWeaponizationSmartphone2022}.

% research goal and structure of the study
% TODO: Add more details about research goal, hypotheses, research questions, scope of the work, brief details about previous works in this field of research, emphasize on new advances, theories and/or applications and include an analysis of results and findings
Therefore, this study attempts to construct security measures for the IT infrastructure, emphasizing the process of collecting and accounting of its internal resources or components for reliability and early insider threat prevention. Firstly, the literature review covers the historical development of IT infrastructure, complexity management, and issues with software composition. Secondly, the methodology specifies the hypotheses and provides a framework for conducting customizable analyses within sandboxed environments. Thirdly, the results part examines features of the proposed solution. Lastly, the conclusion summarizes findings and potential drafts for future work.

\section{Literature review}
\subsection{Historical development of IT infrastructure}
The computing ecosystem and its service models have evolved gradually to on-premises and cloud paradigms, becoming increasingly commercialized to meet customer demands and scaling issues, as broadly represented in Figure \ref{fig:developmental_milestones_of_computing_environment}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_1}
    \caption{Developmental milestones of computing environments (VM - virtual machine; C - container; PaaS, IaaS, SaaS - external services; 1 - mainframe; 2 - cluster of servers; 3 - geographically distributed servers with virtualization (visually represented by color difference); 4 - hybrid approach with cloud services)}
    \label{fig:developmental_milestones_of_computing_environment}
\end{figure}

Initially, there were only mainframes that required dedicated teams for maintenance and performed at a highly mechanical level, so their bulkiness and slow speed enforced considerations for the adaptation of distributed computations \cite[Sec. 3.1]{lindsayEvolutionDistributedComputing2021}. Fortunately, improvements in IT manufacturing significantly contributed to emergence of dedicated servers that could be networked together in a modular way to provide scalability. Following this trend, virtualization moved beyond bare-metal deployments by allowing isolation layers to securely run multiple virtual machines (VMs) on shared hardware to significantly enhance efficiency of resource utilization \cite{randalIdealRealRevisiting2021}. Eventually, increasing demand for international connectivity and economic globalization naturally formed large-scale data centers with network of geographically distributed servers in order to optimize content delivery and processing \cite[pp. 9--11]{lindsayEvolutionDistributedComputing2021}. 

Nowadays, organizations can offload many responsibilities and parts of their IT infrastructure to cloud and service providers which abstract physical machines and offer instant on-demand resource provisioning and management, thereby reducing operational burdens via "as-a-service" models with specialized offerings: software as a service (SaaS), platform as a service (PaaS), infrastructure as a service (IaaS) \cite{mellNISTDefinitionCloud2011}. Software engineers often take that for granted, underestimating interdependence of software and hardware sides for facilitating the execution of general-purpose tasks within multiple layers of abstraction. Such service-based approach for outsourcing still requires integrating and coordinating operations over delegated parts (e.g., security policies, privacy compliance, and external monitoring). Moreover, certain strategic systems (i.e., governmental and military) and their data are prohibited from being hosted in the cloud due to secrecy laws and regional restrictions \cite{arifComprehensiveSurveyPrivacyEnhancing2025, alghofailiSecureCloudInfrastructure2021}.

\subsection{Complexity management with codification and modeling}
As the sector of IT infrastructure has expanded, its complexity and associated challenges have emerged accordingly. Nonetheless, this complexity has become the primary factor for the process automation against manual, repetitive, and impractical operations. Therefore, automation and orchestration are no longer optional - they are critical for leveraging fully scalable and reliable systems from a strategic perspective. One of the natural strategies for complexity management is code. The process of codification captures procedures, routines, or algorithms of actions in a textual format \cite[pp. 1--2]{ouriquesRoleKnowledgebasedResources2023}. For example, Chuprikov et al. propose specifying security policies in IT systems in a form of domain specific language (DSL) that can be ran within an execution engine \cite{chuprikovSecurityPolicyCode2025}. Codification also established the paradigm of Infrastructure-as-Code (IaC) which is focused on specifying the state and content of IT infrastructures to enable reproducibility (e.g., provisioning, replication, copying, scaling) and modification (with emphasis on transparency and maintainability), as well as management and maintenance by means of self-documenting code or files that incorporate defined metadata, configurations, and procedures \cite{pahlInfrastructureCodeTechnology2025}. It minimizes human errors and establishes immutable infrastructure with the phoenix model, allowing any server instance to be recreated, replaced, updated, or destroyed from code without losing critical configurations. This approach eliminates the existence of snowflake servers, which are deviated systems with configuration drift and unpredictability due to their manual modification and absence of documentation \cite[Ch. 2]{morrisInfrastructureCodeDynamic2020}.

IaC operations are managed by system administrators, DevOps teams (Development and Operations), SREs (Site Reliability Engineers), or IT professionals via IaC toolkits (e.g., Terraform, Ansible, etc.), enforcing the utilization of definition files (e.g., templates, playbooks, manifests, etc.), configurations, or execution steps for servers, networks, databases, installations, updates, backups, and so on \cite[Ch. 4]{morrisInfrastructureCodeDynamic2020}. They might be implemented internally via general-purpose languages, but using configurations as application programming interfaces (APIs). For example, containerization tools like Docker use Dockerfiles to define application dependencies and versions, while the runtime builds a container with artifacts according to specified requirements, as shown in Figure \ref{fig:self_documenting_system}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_2}
    \caption{Processes of applying changes to servers (1 - manually; 2 - via IaC pipeline as a self-documenting system)}
    \label{fig:self_documenting_system}
\end{figure}

Any IaC code should be proceeded through the continuous integration and continuous delivery (CI/CD) pipeline, which maintains safe deployment by runtime verification tests within simulated conditions for dynamic analysis and automated scans with validation checks of the codebase for static analysis \cite[pp. 9--10]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}. IT specialists emphasize that IaC codebases should be maintained in accordance with secure coding standards on a regular basis (e.g., ISO/IEC series, NIST frameworks, etc.) \cite{konalaFrameworkMeasuringQuality2025}. According to Saavedra et al., incorporating intermediate representation (IR) and source-to-source translation into IaC static analysis facilitates the development of language-agnostic methods that can be ported to any IaC format \cite{saavedraPolyglotCodeSmell2023}.

Sandobalin et al. examined the advisability of using IaC modeling tools by training control groups on Argon and Ansible \cite{sandobalinEffectivenessToolsSupport2020}. The research showed that Argon, as a visual toolkit, is easier to learn and can abstract complexity as a thin layer above existing IaC configuration formats. Thus, allowing users to simply draw their IT architecture, while the underlying system automatically generates a specific configuration for deployment. It implies that complexity management and computational thinking also extend to graphical and other representations of information. Diagrams and visualizations provide two-dimensional or multidimensional representations, enabling more structured modes of reasoning, whereas text is inherently a linear representation of information. Certainly, these forms should not be regarded as mutually exclusive, rather they synergize with each other and are interchangeable at different levels of abstraction. Furthermore, Model Driven Engineering (MDE) methodology proposes treating documentation, specifications, and model definitions as primary and reference artifacts describing behavior and components of a system, from which code generation toolkits can facilitate transformation between them (e.g., into source code templates in any programming language or pseudocode) in order to achieve synchronization \cite{verbruggenPractitionersExperiencesModeldriven2023}. Firstly, it bridges the gap between developers and business-oriented specialists by demonstrating certain domain-specific problems and rules, so they can focus more on business logic. For instance, Alfonso et al. developed the BESSER platform based on MDE, which provides a canvas editor and multiple design notations like UML (Unified Modeling Language) for generating backend applications from diagrams \cite{alfonsoBuildingBESSEROpensource2024}. Such toolkits usually support the development of reliable prototypes and concepts, but it is not intended for high-load or performance-critical systems. Secondly, MDE allows structures to dynamically adapt to emerging requirements instead of manually rewriting boilerplate code and thinking about platform-specific implementations, so the development process is optimized. For example, it is applicable for automatic propagation of components changes and migration between layers instead of repetitive manual transformations, conversions, and modifications \cite{chillonPropagatingSchemaChanges}. In this regard, AI in the form of language models enables rapid code generation from specifications as detailed prompts and instructions \cite{diroccoUseLargeLanguage2025, stoicaSpecificationsMissingLink2024}. Although generative language models can cause challenges due to hallucinations and their stochastic nature, it might be partially mitigated through enforcing strict rules, validation mechanisms, and multiple series of output refinements \cite{shankarWhoValidatesValidators2024}.

\subsection{Resource management and accounting}
% TODO: extract more valuable information from given references (sabettaKnownVulnerabilitiesOpen2024, SharedVisionSoftware, jaatunSoftwareBillMaterials2023) in order to extend this part
OWASP (the Open Worldwide Application Security Project) identified 10 major risks for IT infrastructure which 5 of them are directly related to IT resources accounting \cite{OWASPWwwprojecttop10infrastructuresecurityrisks2025}: "Outdated Software", "Insufficient Threat Detection", "Insecure Resource and User Management", "Insecure Access to Resources and Management Components", "Insufficient Asset Management and Documentation". This fact clearly emphasizes that proper management of all system components and organizational knowledge is mandatory. In this context, software composition analysis (SCA) is a structured approach for collecting and accounting software components, defining common specifications and governance principles \cite{sabettaKnownVulnerabilitiesOpen2024}. It facilitated the establishment of a widely adopted format for software bill of materials (SBOM) with its multiple variations that allows to reduce a total vulnerability response time \cite{SharedVisionSoftware, jaatunSoftwareBillMaterials2023}. Other progressive solutions can assist in the same task by collecting data from a running production instance to subsequently generate recreatable configurations and identical images/snapshots \cite{kleinInfrastructureCodeFinal}. Usually, such procedures are combined with version control systems that store and track versions of all project files, allowing teams to have: process of collaborative development, a single source of truth, and well-defined compliance by localizing areas of responsibility \cite{ragavanVersionControlSystems2021}. Particularly, connections and links between system components form "dependencies" which have direct and indirect involvement on certain functionalities and qualities of base system. As demonstrated in Figure \ref{fig:components_of_complex_system}, OS (operating system) can be illustration of complex system requiring correct operation of its components providing emergent qualities.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_3}
    \caption{Components of a complex system}
    \label{fig:components_of_complex_system}
\end{figure}

According to statistics of the European Cyber Security Organisation (ECSO), solution-specific logic often represents only about 10\% or less of an application's total code, while the remainder is largely made up of third-party dependencies such as software (module, package, library), service (requests to external systems and network API), infrastructure (like drivers and underlying execution platform) \cite[p. 8]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}. Given a script in the Python programming language, the internal dependency in this case is the self-contained code, while the external dependency is everything that was created by third-party such as implementation of standard library, interpreter/compiler, specification, OS or platform-specific requirements, and so on, as shown on Figure \ref{fig:python_ecosystem_from_perspective_of_dependencies}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_4}
    \caption{Ecosystem of Python programming language from perspective of dependencies. Adapted from \cite[p. 8]{ECSOWG6TechnicalPaperSoftwareSupplyChainSecurity}}
    \label{fig:python_ecosystem_from_perspective_of_dependencies}
\end{figure}

From a broader perspective, the modern economy is service-oriented and highly interconnected in the global supply chain, where everyone relies on each other, as represented in Figure \ref{fig:example_of_global_supply_chain}. Hence, engineers can mitigate issues of resource constraints (e.g., schedule, personnel, risks, requirements, finance, and qualifications) by integrating external dependencies and reusing code as outsourcing for common problems during the implementation and maintenance of IT products.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_5}
    \caption{Simplified example of the global supply chain in the IT context}
    \label{fig:example_of_global_supply_chain}
\end{figure}

\subsection{Compromise and trust} 
% TODO: edit this section and extend it with (algorithms, tables, figures) and translate to academic english
% TODO: write more details to already created bullet points with detailed explanation

% supply chain attacks and dependency theory
% TODO: нужны примеры supply chain attack инцидентов в виде таблицы с перечислением (насколько увеличились в динамике и т.д.)
% TODO: reference to constraint satisfaction problem and dependency theory
On the other hand, any risks and failures in a single link can cause a cascading effect across the entire chain which is known as a supply chain attack. This attack vector exposes the ability to implicitly or explicitly influence on any IT infrastructure by leveraging direct and indirect (i.e., transitive) dependencies, invoking instability and fragility \cite{themitrecorporationSupplyChainCompromise2025}. It can be caused by any potential way of external influence, including unintentional flaws and defects, intentional threats like malicious software (malware), or destabilizing factors like breaking API changes and internal modifications \cite{okaforSoKAnalysisSoftware2022}. Неакуратное/небрежное обращение с dependencies в команде может стать одной из причин подверженности to supply chain attacks. Поэтому необходимо знать об разных dependency management подходах таких как: % TODO: reference
\begin{itemize}
    \item Manual - vendoring за счет копирования external dependency внутрь своей кодовой базы для поддержания и изменения в ручном режиме.
    \item Automatic - dependency/package managers as отдельное software that downloads, handles, updates, and делает другие операции с зависимостями в проекте.
\end{itemize}
Их можно комбинировать и применять совместно с теме же build specification files (cmake, make, setuptools, etc.). Хотя некоторые считают что manual one дает более детальное понимании содержания зависимостей (ведь исходники храняться вместе с нашим кодом в виде source code или собранных файлов) и больше контроля (like frozen/pinned то есть мы обновляем когда сами явно делаем это), оба подхода одиннаково подверженны supply chain атакам из-за того что транзитивные зависимости все равно can propagate through. Следовательно те пакеты которые явно не закреплены не желательно использовать, ведь они могут в любой момент пропасть или замениться другими из списка direct пакета из которого они идут. Атакующие пользуются как раз этими и другими особенностями в свою пользу. К примеру, некоторые взлоумышленики abuse identity confusion чтобы походить на другие пакеты (по названию, описанию, и другим метаданные), отжимать не зарегистрированные имена, и другие операции чтобы запутать инструменты или пользователей \cite{shradhaBeyondTypeSquatting291044}. Поэтому очень важно знать об underlying принципах работы package manager при установки какой-либо группы зависимостей such as dependency resolution/resolving and their parsing. From theoretical standpoint, dependency resolution всего лишь являются вариацией на тему constraint satisfaction problem, суть которой в попытке to satifsy определенному заданному набору условий и правил (similar to game rules / game theory). Оно является NP-hard problem где нет оптимального алгоритма для решения, что выражается в том что вычиление дерева/графа зависимостей усложняется каждый раз при добавлении one more dependency and условий (version, limits, и другие constraints которые зависят от конкретной экосистемы) как показано на Figure \ref{fig:dependency_theory}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_6}
    \caption{Dependency theory (1 - требования по версии или времени публикации пакета from initial publised version to latest one, такие пакеты различных версий являются вариациями одной сущности, может применяться разные versioning formats; 2 - dependency group from set theory который ограничивает или увеличивает кол-во пакетов необходимых для загрузки (некоторые могут быть обязательными или optional); 3 and 4 - вложенность зависимостей за счет direct and transtive ones, в реальных проектах зависимостей и условий больше что приводит к dependency conflict/hell from graph theory (cross references, circular/recursive, cycles/loops, duplicates, etc.) с которым надо deal with (e.g., flatten tree of linear structure))}
    \label{fig:dependency_theory}
\end{figure}

\begin{comment}
% TODO: оформить как https://en.wikibooks.org/wiki/LaTeX/Algorithms
% TODO: pseudocode for resolving dependencies and backtracking, handle RecursionError
% TODO: references
- https://github.com/pypa/pip/tree/36987b0c31b97ffb9fb7949ded628e9a6b10c016/src/pip/_vendor/resolvelib
- https://pip.pypa.io/en/stable/topics/dependency-resolution/
- https://pip.pypa.io/en/stable/topics/more-dependency-resolution/
Выстраивание полного дерева зависимостей, разобрав работу “uv tree” и похожих инструментов для того чтобы понять как получить данные обо всех зависимостях (базовый алгоритм dependency resolving)
- устанавливаем определенную зависимость (e.g., matplotlib с указанием версии или условий этой версии)
- зависимость находится в переходном состоянии, пока не будет закреплена (“freezed”, “pinned”) и установлена
- для закрепления необходимо провести dependency resolution of dependency graph
    - найти все зависимости данной зависимости (прямые зависимости)
    - у каждой зависимости этой зависимости есть зависимости ниже и так далее (транзитивные зависимости)
    - но при такой рекурсивной проходке возникают dependency / circular dependency (когда одна зависимость зависит от другой или есть конфликт по определённым требованиям). Есть множество решений данной проблемы: дублировать скачиваемые зависимости но с другими версиями и условиями, и т.д.
    - К примеру, matplotlib имеет pillow в прямых зависимостях, но в свою очередь pillow имеет ещё множество зависимостей и так далее рекурсивно…
    - В идеале, хотелось бы дойти вглубь до кода который существует без зависимостей (внешних библиотек или установки), но это невозможно практически учитывая что есть stdlib python или libc которые тоже формально зависимости. Также надо учитывать, что есть зависимости которые не отмечены в pypi но они формально являются зависимостями (обертки над кодом C библиотек или кодов из другой экосистемы)
% TODO: упрощенный алгоритм парсинга dependencies (поиск всех файлов с requirements and всех зависимостей вниз но это не резолвинг сам по себе а лишь парсинг)
\end{comment}

% compromise of trust in world of convenience/usability vs security (functional vs non-functional requirements)
% TODO: extract more details from given references
Многие software repositories нацелены на функциональные возможности чем на безопасность, прикрываясь мотивами что они не хотят ограничивать доступ community которое open for everyone (считая что security is overhead) \begin{comment} TODO: reference sasseDebunkingSecurityUsabilityTradeoff2016 \end{comment}. Тем самым они подвергают себя и других риску что everyone can affect maliciously to ecosystem. Так что пользователям приходится самостоятельно бороться against chaos внешнего мира. Current market solutions cannot be considered as ultimate against supply chain attacks, because gathering reliable data from software is challenging due to its distribution formats and categories: % TODO: reference
\begin{itemize}
    \item white box - fully known content (e.g., open source);
    \item gray box - partially known content (e.g., leaked information, rumors, reverse engineering, insider knowledge, alternative products, or forks);
    \item black box - unknown content (e.g., functionality behind API like SaaS, proprietary and closed-source software).
\end{itemize}
SCA typically focuses only on white box, gathering already indexed vulnerabilities or components, thereby leaving critical gaps in threat visibility and transparency. This is understandable, since other categories are inherently more complex due to specifics, because many companies use anti-analysis techniques to protect their software and intellectual property, making component-level inspection difficult or reasonably impossible \cite{zaidenbergTrulyProtectVirtualizationBased2022} \begin{comment} переписать что такой-то такой то автор приводит примеры обхода анализа and extract more details (такой же референс переиспользовать Results) \end{comment} Despite rising SCA adoption, multiple researchers indicate that supply chain vulnerabilities and incidents continue to increase, and the remediation process often remains slow \cite{senDeterminantsSoftwareVulnerability2020, jafariDependencyPracticesVulnerability2023}. Moreover, Alfadel et al. found that, in the Python ecosystem, it takes a median of up to 4 months to fix all occurrences of a security vulnerability in the PyPI repository \cite{alfadelEmpiricalAnalysisSecurity2021}. Experts also identify several additional drawbacks and limitations of existing SCA tools \cite{dietrichSecurityBlindSpots2023, enckTopFiveChallenges2022, bohmeSoftwareSecurityAnalysis2025}:
\begin{itemize}
    \item focusing more on static analysis rather than on dynamic analysis during runtime execution;
    \item they operate within the local environment instead of isolated or remote setups;
    \item comprehensive deep scanning of transitive dependencies is often missing;
    \item insufficient integration with AI and data analytics-based systems.
\end{itemize}
Нынешняя слишком разрозненная экосистема of software solutions только усугубляет эту ситуацию, ведь множество пакетов и непонятно кто будет нести ответственность и придерживаются ли такие maintainers стандартов. Even software package that is trusted by the community cannot offer proper guarantees, because mainstream software licenses clearly mention that their products are distributed "as is" condition \cite{wintersgillLawDoesntWork2024}. Эксперты предлагают различные варианты решения текущего статуса от слабых до радикальных мер. Во-первых, следует вводить повышение обязательных и строгих базовых требований (baselines) на уровне платформ с гарантиями их соблюдения пользователями, что могло бы сократить число атак и вредоносного кода due to высоким стандартам и улучшенному конечнному качеству. Такой подход напоминает регуляции в продуктах питания или в обращении с водными ресурсами что позволяет сохранить здоровье потребителей. Это также отражает принцип "secure by default", где secure policies and good default configurations должны стать нормой для многих а не исключением только для профессионалов к примеру \begin{comment} TODO: reference \end{comment}:
\begin{itemize}
    \item Добавление обязательный overseeing анализ перед опубликованием пакета и последующий при каждом обновлении/изменении;
    \item Выстраивание системы модерации где комьюнити будет само естественно репортить вредоностные пакеты и реагировать; % добавить референс на Bug Bounty систему также - платить комьюнити за поиск багов
    \item Политики для временной блокировки или выдача "call down" сроков между действиями пользователей при их аномальной активности (смена IP адреса, смена пароля или определенного токена, недавняя публикация изменений в пакет, и многое другое)
\end{itemize}
Secondly, software market should обратить больше внимания и озабоченность for formal regulatory mechanisms to increase trust and responsibility in community such as certification, auditing, and transparent processes, along with initiatives aimed at improving standards. Sammak et al. attended an interview among a small subset of experienced developers, which revealed that the majority of respondents are not aware of software supply chain management or have a superficial understanding due to the absence of proper training on this topic for a broader audience \cite{sammakDevelopersApproachesSoftware2023}. Thirdly, те же popular or operation-critical software have сильное influence on whole landscape, so compromising them can undermine not just dependent infrastructure but the global contracts of security such as cryptography stack \cite{tsoupidiProtectingCryptographicLibraries2025}. Self-replicating malware or virus might compromise a significant portion of the IT ecosystem by ensuring that any code compiled on an infected hardware platform, compiler, or OS becomes compromised as well. \begin{comment} TODO: references to Ken Thompson's hack from other guys like ladisaTaxonomyAttacksOpenSource2022 \end{comment}. This is due to the fact that definition files are merely instructions without guarantees that our intentions will be correctly expressed and transformed with preserved semantics in the produced output (e.g., machine code, process, and other representation) by intermediary system (compiler, interpreter, execution environment), as shown in Figure \ref{fig:design_and_implementation}. \begin{comment} TODO: add reference to https://arxiv.org/pdf/1803.07244 and to programming language model of execution (C++ abstract machine)\end{comment} 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_7}
    \caption{Intermediary system between design and implementation}
    \label{fig:design_and_implementation}
\end{figure}

Futhermore, а если мы захотим провести compromise assessment или software evaluation такого полученного product, то зараженные дебагер или декомпилятор (reverse engineering and deconstruction tools) или чистые их версии запущенные в compromised environment специально будут blinded in some cases to hide зараженную parts and backdoors в ПО. (to be remained undetected).

Поэтому важно осозновать каким рискам подвержены и нужно тщательно понимать чему и кому нам доверять
Trust это не статичный и точный показатель, это больше как probability of someone being more trusted and being trusted by someone else (процент доверия других к нему и то что он не обманет с определенной вероятностью других) (и trust очень дорогое - любая ошибка ведет к ее моментальной потере с обоих сторон). % TODO: reference \cite{singerTrustBasedSecurityTrust2020} - Они предлагают рассматривать “trust” в форме relationship модели, где “trust” на самом деле это “когда мы полагаемся и надеемся на third-party без всяких доп. сомнений” (cчитая их trustworthy) что делает нас “vulnerable”. Поэтому они советуют применять моделирование угроз и анализ рисков перед design стадией для минимизации фактора of “trust”. Помимо этого “trust” рассматривается как наши недочеты и assumptions (к примеру, не досмотрели и проглядели ведь доверились какой-то нибудь системе). По сути это можно растянуть на много что еще: интернет протоколы и способы передачи данных (покуда получатель и отправитель будут соответствовать стандартам / интерфейсу взаимодействия то данные будут передаваться к корректной форме) - одна из форм of “trust”

% TODO: add reference leeDoDEnterpriseDevSecOps
% TODO: security as dynamic and adaptive system, attack vectors (add reference pourmoafiSolutionSecuringInformation and mengCybersecuritySimondonsConcretization2022)
% if you are distracted - instead you should быть всегда на готове. as atomic segments, trusted semi trusted and trusted to untrusted, transaction properties (ACID), security posture
National Institute of Standards and Technology (NIST) ввело такую модель как the Zero Trust Architecture, которое organizations необходимо adapt by applying strict comprehensive assessments of vendors, suppliers, and corresponding software supply chains, as any characteristics in external dependencies can impact whole IT infrastructure \cite{roseZeroTrustArchitecture2020}. Babenko et al. clearly demonstrated that empirical data on Zero Trust implementation correlates with a lower incident rate of insider threats \cite{babenkoCybersecuritylevelAssessmentModels}. В блокчейн системах пришли к схожему выводу и сформировали trustless концепции. \begin{comment} TODO: add references and expand about blockchain - zero knowledge proof and proof-of-x \end{comment} Помимо этого эксперты предлагают делать системы таким образом чтобы они были secure by design in their foundation где безопасность встраиваться изначально а не дополнительный элемент (ставить нефункциональные и функциональные качества равными). Конечно, мы избавимся от уязвимостей и других проблем внутри систем как полностью безопасные, но сформируем процесс of multilayered security and defense in depth. Также системы должны быть более комплексными до такой степени чтобы они могли самоанализировать себя и доулучшать as next-generation systems (that can analyze themselves) и делать системы более надежными (less fragile). К примеру, Berhe et al. showed the necessity for a mitigation plan and a regular budget on automated dependency recovery or manual actions such as replacing it with alternatives, upgrading it, or reverting to a previous release \cite{berheMaintenanceCostSoftware2023}. Если мы возьмем well-decomposed systems might still continue to function in such cases, or have recovery mechanisms ("self-healing") for the failure of individual components only up to a certain limit, after which the entire system becomes non-operational or some functionality is temporarily disabled \cite{dehrajReviewArchitectureModels2021}.

% Add reference to enablingGlobalCollaboration
Finally, нам необходимо замедлится как индустрии и переосмыслить то что мы уже произвели (найти закономерности, повторения, и т.д.) - we need to stop перепроизводство 	в 	software engineering and start recycling of software waste (recycling of old and заброшенных ideas) by переосмысливать их. 
В-третьих, standard library (stdlib) must be rich and better with batteries included которые регулярно улучшаются и меняются в официальной поставке. External dependencies can cause vulnerabilities, but in other hand they are providing already made solution for some problem and very easy to use / apply (две стороны монеты). И есть stdlib is weak, то на место него появляется множество конкурирующих dependencies на одну определенную тему. For example, целый зоопарк решений as multiple JS frameworks for frontend (React, Vue, Angular, etc.) которые по сути делают одно и тоже и решают одни те же проблемы. 
Однако, пока нам приходится работать с тем что есть в текующей парадигме. 
Mirakhorli et al. reviewed 84 available SBOM management toolkits across multiple programming language ecosystems, which demonstrates the lack of unification and significant fragmentation in current solutions \cite{mirakhorliLandscapeStudyOpen2024}.

% TODO: расширить эту часть подробнее как подведение итогов and identified research gaps
There is a practical problem with current industry approaches, requiring immediate innovations. While specifications are important, implementations of toolkits must also meet high-quality standards.

% ---------------------------------------------------------------------------
\section{Methodology}
% add table to literature review (сравнение с другими решениями) and results (анализ рисков).
% Добавить рефренс про сандбоксинг из NIST zero trust в начало методологию (где говориться про сандбоксы)
% привести приме как работают элементы других систем прежде чем раскрывать свою
% Показывает SOTA solutions раскрывая аспекты и характеристики, постепенно вычленяя определенные элементы которые слабо-развитые в его контексте - и постепенно тем самым переходит на методологии за счёт нахождения gaps и предлагает свои решения по этому повод (в конце лит обзора подводит итоги с переходом на методологию)
% TODO: Принцип работы современных с технической стороны
% add table to this part and literature review
% TODO: pseudocode в практической части для своей системы помимо блок-схем
% TODO: add about virtualization and digital twins (maybe to methodology briefly)
% TODO: Decompose to more subsections (depending on problem and components not just one large section). Методы (методология исследования) – методы, используемые для проверки гипотезы, должны быть описаны достаточно подробно, чтобы другой исследователь в этой области мог повторить проверку.
% Перенести все про виртуализацию и digital twins в методологию (concept explanation) типа будет мини лит обзор по этой теме, но в самой диссертации эту часть нужно подробно раскрыть ведь у виртуализации есть свои многие недостатки которые необходимо решить
% @TODO: refine methodology, results, conclusion
% comparative analysis, механизмы взаимодействия компонентов, portable/technology-agnostic
% They are also not an absolute solution, multiple underlying issues with sandbox also
% @TODO: expand this part and write more structured way
% Тут также добавить про то что системы будет открытой и любой сможет поучаствовать в развитии проекта в дальнейшем и его философия (что его отличает от конкурентов)
% добавить алгоритмы вместе с псевдокодом и UML diagrams и т.д. в методологию
The methodological part focuses on addressing problems and gaps identified in the literature review of the current landscape. A comprehensive lifecycle was developed, accompanied by essential architectural decisions.

\subsection{Concept and paradigm shift}
It was decided to explore principles from existing malware and binary analysis tools, such as sandboxes. These toolkits can effectively complement modern SCA approaches and enable a paradigm shift, as illustrated in Figure 6. The proposed idea involves redirecting the purpose of existing solutions toward a different target audience and analytical focus from regular end users to developers. Although this may appear to be a minor shift, it is both critical to have and straightforward to integrate.
% @TODO: need references and examples of such toolkits
% Тут доп. пояснение про задачу зависимостей (dependency theory and so on). И доп детали нормально что из себя представляет методология и т.д.
% Такой подход тоже не идеальный и у него есть свои limitations, но позволяет именно искать глубокие зависимости на самом деле а не просто просматривать поверхностно содержание
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{} % figure_6
    \caption{Paradigm shift by applying malware and binary analysis for SCA-related tasks} % TODO: also about simulations and digital twins
    \label{fig:placeholder}
\end{figure}
Sandbox and automated malware analysis enable examination of software within isolated, reproducible execution environments. They also support custom scripted scenarios that emulate developer behavior when interacting with specific libraries or applications (e.g. according to use cases or user stories), thereby revealing hidden during runtime. These tools offer a rich set of functionalities and methodologies that are highly relevant and should be leveraged for advancing software dependency analysis.

\subsection{System modeling}
The problem domain was decomposed into several subtopics that naturally align with required components and services, ensuring that it remains both extensible and customizable. The architectural design was based on the principles of hexagonal architecture, because it provides a clear separation between supplementary elements and the core business logic, such as driving side, application, and driven side. The model introduces the concept of adapters and ports, borrowed from electronic engineering, which enables the replacement or modification of component implementations. Firstly, the driving side represents the system’s entry points, including human users and external automated systems that interact with the platform through backend APIs and web interfaces over network connections within client-server model, as clearly shown in Figure 7.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_7}
    \caption{Driving side of the architecture}
    \label{fig:placeholder}
\end{figure}

Secondly, the internal part of the application is designed as a modular monolith, divided into multiple components or services, as illustrated in Figure 7:
\begin{itemize}
    \item “Identity provider, user and session management” – maintains all information about users in the platform with their metadata. Generally, “Authentication” and “Authorization” operate alongside with that, forming a comprehensive set of components responsible for user-related security operations: roles or access control based on permissions, login and register handling, access/refresh token management, checking behavior and actions of user, multi-factor authentication (MFA), and so on.
    \item “Groups and project system” – holds multiple analyses in which users can collaborate with each other.
    \item “Moderation system” and “Administration system” – are intended for platform operators, enabling internal system management and response to user requests as needed.
    \item “Analysis system” – is a critical component that performs all essential operations required for software analysis.
\end{itemize}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_8}
    \caption{Internal application components of the architecture}
    \label{fig:placeholder}
\end{figure}

Components can be added, modified, or disabled as needed. This flexibility improves security by reducing the attack surface, minimizing system states, decreasing computational complexity, and factor of combinatorial explosion. The analysis engine also features a plugin and modding system with multiple operating modes. This means that users of the system can develop their own plugins for specific use cases by implementing the provided interface. Such extensibility enables the analysis not only of software packages but also of container images, computer networks, and other parts of IT infrastructure. Furthermore, there is support for fine-grained configuration and operation modes. For example, specifying which data to collect or ignore, potentially through regex-based filtering.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_9} % Redraw as UML Class diagram short with bundle example for plugin system
    \caption{Modification of system}
    \label{fig:placeholder}
\end{figure}

The analysis engine consists of tightly coupled contextual components, all operating around a common abstraction of “task”:
\begin{itemize}
    \item Orchestrator — is responsible for task queuing, scheduling, and overall management operations. It serves as an intermediate layer that receives tasks directly from the analysis system, each with specific requirements that must be resolved before delegation to other components. At the same time, it manages and coordinates the overall execution process, acting as the central communication and control hub, because components in the Bundle cannot begin execution without confirmation or explicit request from Orchestrator. In this context, Orchestrator interacts with the Bundle abstraction rather than directly with its individual components, which contains a set of predefined task executors as described below. Also, the connection between Orchestrator and Bundle follows a one-to-many relationship, where Orchestrator determines which Bundle to use among several available ones.
    \item Instancer (i.e., Provisioner) – creates, prepares, and manages the required environments (e.g., containers, VMs, specific OS or images, etc.).
    \item Collector (i.e., Extractor) – continuously or periodically collects data from created instance (e.g., logs, files, making image screenshots or recording videos, memory or disk snapshots, etc.).
    \item Interactor – performs predefined set of actions or scripted scenarios in the isolated environment to simulate user behavior and trigger malware responses (e.g., interaction with file system, launching or installing programs, managing input/output devices, etc.).
    \item Parser – transforms the required data into a unified format, such as an intermediate representation (IR), or converts it into a format suitable for Analyzer.
    \item Analyzer – produces specific verdicts or predictions based on the received data (e.g., determining critical components within dependency tree, performing other specialized analyses using both stochastic and deterministic algorithms).
\end{itemize}

Thirdly, Figure 8 illustrates the components of the driven side along with some generic infrastructure elements that are required for proper application functionality. % @TODO: описать за что каждый компонент из driven side отвечает и зачем он там
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{}
    \caption{Driven side of the architecture}
    \label{fig:placeholder}
\end{figure}

Overall, the prototype demonstrates a variation that can be implemented independently of specific technologies, making it a technology-agnostic and portable solution. % вместо завершения дальше продолжить и представить алгоритмы

\section{Results}
% TODO: Результаты – гипотеза должна быть проверена, а данные, представляющие результаты проверки, представлены. (обязательно графики и таблицы с результатами данных). результат это алгоритм, метод, модель, архитектура для статьи и также обоснование
The results include an analysis of security and critical decisions derived from the proposed methodology. %  as well as a comparison with existing market solutions addressing the same topic.
\subsection{Threat modeling and risks}
% TODO: Добавить про потенциальные риски также
The analysis emphasized the external perimeter while also exploring specific elements of the PyPI platform, which already demonstrates several comparable and thematically relevant aspects. It hosts a large number of different Python packages and is owned by the Python Software Foundation (PSF) which operates under a free and open model, where any user may use the package manager or other mechanisms to upload, download, and publish packages in the form of source or build distributions. Similar systems and policies are followed by other software repositories and ecosystems, such as Maven Central, NPM, Crates, and many others. 
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{}
    \caption{DFD legend}
    \label{fig:placeholder}
\end{figure}
Also, there was prepared the DFD (data-flow diagram) legend with all necessary elements for the drawing process, as represented in Figure 9. Moreover, constructed level 0 DFD as shown in Figure 10. It was consciously decided to omit aspects concerning connectivity to the internet (e.g., internet service providers and intermediary network components), as those elements fall outside the scope of this analysis.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{} % span across 2 columns wide at the start of page
    \caption{Level-1 DFD (interaction interfaces with the platform)}
    \label{fig:placeholder}
\end{figure}

\subsection{Tradeoffs in the architecture prototype}
This section outlines the critical architectural decisions made and the reasoning behind them. Firstly, it was decided to keep the Parser and Analyzer separate to ensure better decomposition; however, merging them into a single entity could improve performance and efficiency. Secondly, to reduce the load and prevent bottlenecks on a single Orchestrator, multiple instances of the Engine should be deployed, or the system should run in a concurrent or parallel mode. Thirdly, it was decided to group task executors into a single Bundle, as these components were found to be highly interdependent. Initially, they were considered as separate entities without direct communication, following a microkernel architecture in which component interaction occurs asynchronously (via message bus and inter-process communication). However, this approach proved unsuitable, since Collector depends on the specific implementation of the data source, Parser requires awareness of data format, and Analyzer cannot detect anomalies or draw conclusions from incompatible input data.

\section{Conclusion} % Заключение – данные должны быть обсуждены, результаты интерпретированы и сделаны выводы.
The topic of supply chain management in IT infrastructure remains highly relevant, as the number of related incidents continues to grow despite the emergence of new tools and standards. More fundamental, preventive measures may be required at the source level, where software packages are produced, rather than reacting after incidents occur.

Future research will focus on converting the proposed methodology into a microservice-based system, which remains a challenging task due to its complexity and management requirements. However, such an approach would enable scalability and continuous integration of new functionality without interrupting the platform. Further work will also include a full implementation of the system and an independent statistical analysis of software dependencies beyond those examined in the current literature review.

% \nocite{*}
\printbibliography % deduplication and autoformatting https://tex.stackexchange.com/questions/233086/avoiding-duplicate-entries-in-bibliography-having-different-cite-keys, Also check if rendering is right for IEEETran and links also represented https://libraryguides.vu.edu.au/ieeereferencing/technicalreports

\end{document}
